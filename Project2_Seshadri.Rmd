---
title: "Auto Insurance prediction"
author: "Sri Seshadri"
date: "10/20/2017"
output: 
  pdf_document:
    toc : true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F,tidy = T )
```

\pagebreak

# 1. Introduction

An insurance company is interested in predicting which customers are likely to be in an accident and what would be the likely payout. The company requires this prediction to price the insurance policy. A predictive model is required to be deployed at point of request for quote or sale. The insurance company has been collecting data on which a predictive model would be trained and tested. 

## 1.1 Analysis Process

The following process steps were used for building a predicitve models:

* Exploratory Data Analysis
    + Perform data quality checks, quantify missing data.
      - Check for systemic loss in data
    + Understand relationships amongst predictors and between target variables and predictors.
      - Create attribure or indicator variables to aid data cleaning.
      - Filter out clean data for feature selection and model building.
* Feature Selection
    + Subset complete records to model wins in season
      - Use different modeling techniques to select candidate predictors.
      - If data is missing for candidate predictors, identify imputing methods.
* Model Building
    + Test models that were build using complete records on the entire data set with imputed data.
    + Compare models based on Deviance, ROC and MAE
      - Check if models make physical sense. 
* Initial model deployment
    + Deploy model to predict wins on out of sample data.
    + Discuss models and results with subject matter experts.
    + Fine tune model and re-test
* Final model deployment
  
    
## 1.2 Executive summary


# 2. Data

The insurance comnpany has data collected from almost 8200 customers. The dictionary of the data is provided in the appendix A.1. Tables 1 and 2 show the summary statistics of numeric and non-numeric features of the data. It is seen that some features have missing values. The missing values may need to be imputed if the features are deemed important predictors of likelihood of customer involving in a crash or payout. 

It is seen that the minimum age of car is -3. Which is not rational, the data is filled in with +3 assuming it is a typographical error. Also it is seen that there is white space in the JOB column of the data. 

```{r Import}
# import data
df <- read.csv('logit_insurance_rev.csv')

# Sanity Check of numeric variables
library(mosaic)
sanitycheck <- do.call(rbind,dfapply(df,favstats, select = is.numeric))
knitr::kable(round(sanitycheck,2), caption = "Summary statistics")

# Sanoty check on non-numeric variables
sanitycheckcharacter <-select(df, colnames(df[1,sapply(df,class) == 'factor']))
library(purrr)
UniqueVals <- sanitycheckcharacter %>% 
  map(unique)

Counts <- data.frame(sapply(UniqueVals,length),
                     do.call(rbind,dfapply(df,length,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,n_missing,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,function(x)length(x[which(x == " ")]))),
                     row.names = names(UniqueVals))
colnames(Counts) <- c( "# Unique", "n","missing","Blanks")
knitr::kable(Counts, caption = 'Sanity check of non numeric variables')

# Creating attribute columns for missing variables
df.rev <- df %>% 
            dplyr::mutate(YOJ_Missing = ifelse(is.na(YOJ),1,0)) %>% 
            dplyr::mutate(Income_Missing = ifelse(is.na(INCOME),1,0)) %>% 
            dplyr::mutate(HOME_Val_Missing = ifelse(is.na(HOME_VAL),1,0)) %>% 
            dplyr::mutate(CAR_AGE_Missing = ifelse(is.na(CAR_AGE),1,0)) %>% 
            dplyr::mutate(AGE_Missing = ifelse(is.na(AGE),1,0)) %>% 
            dplyr::mutate(CAR_AGE = ifelse(!is.na(CAR_AGE) & CAR_AGE <0, abs(CAR_AGE), CAR_AGE))

# save file for external data analysis
# write.table(df.rev,'insurancedata.csv')

# Defining predictors and temporarily removing Target amount from the data
predictors <- colnames(df.rev)[which(colnames(df.rev) %in% c("INDEX", "TARGET_AMT"))*-1]
df.rev.cleaned <- df.rev %>% 
                    dplyr::select(c('TARGET_FLAG',predictors)) 
write.csv(df.rev.cleaned, file = 'dfrevcleaned.csv')
# creating indicator variables for factor variables

factorcols <- sapply(df.rev.cleaned,is.factor)
dummyvars <- dummy::dummy(df.rev.cleaned[,factorcols],int = T)
df.rev.cleaned <- cbind(df.rev.cleaned,dummyvars)
#df.rev.cleaned.comp <- df.rev.cleaned[complete.cases(df.rev.cleaned), c(- 1,-7,-23:-27)]          
```

## 2.1 Exploratory Data Analysis (EDA)

In this section intresting observations in the data are noted and used to characterize the population of customers. 

### 2.1.1 Customer profile

Figure 1 shows the customer portfolio of the insurance company. It is seen from figure 1 that majority of the customers hold blue collar and clerical roles. The income and home values are zero inflated, likely contributed by students and home makers. Nearly 25% of the customers have been involved in a car crash. Most of the customers are with the policy less than 2 years. 

 
```{r, fig.cap="Customer portfolio"}
library(forcats)
library(ggplot2)
cp <- ggplot(data = df.rev.cleaned)
cpbar <- cp + geom_bar(mapping = aes(fct_rev(fct_infreq(JOB)))) + theme_classic() + scale_x_discrete(drop = FALSE) + xlab("JOB") + coord_flip()

cpage <- cp + geom_histogram(aes(x = AGE)) + theme_classic()
cpincome <- cp + geom_histogram(aes(x = INCOME)) + theme_classic()
cphomeval <- cp + geom_histogram(aes(x = HOME_VAL)) + theme_classic()
cpTIF <- cp + geom_histogram(aes(x = TIF)) + theme_classic() + xlab('Time in force')
cpRisk <- cp + geom_bar(aes(x=TARGET_FLAG)) + xlab("Involved in crash - Yes (1) or No (0)") + theme_classic()

gridExtra::grid.arrange(cpbar,cpage,cpincome,cphomeval,cpTIF,cpRisk, nrow = 2, ncol =3)
```

It is seen that majority of the customers own cars that are new; less than 2 years old. The older cars are owned by professionals and by the group who has their nature of job missing (potentially not disclosed). Also interestingly that cars that are between 14 and 15 years of age are missing from the population. As seen in histogram in figure 2. 

```{r, fig.cap="car age"}

p <- ggplot(data = df.rev.cleaned[!is.na(df.rev.cleaned$CAR_AGE),])
caragehist <- p + geom_histogram(mapping = aes(x = CAR_AGE)) + theme_bw() + xlab("Age of cars")
caragebox <- p + geom_boxplot(mapping = aes(y = CAR_AGE, x = fct_reorder(JOB,CAR_AGE), color = JOB, fill = JOB),alpha = 0.25) + theme_classic() + theme(legend.position = "none") + coord_flip() + xlab("JOB") + ylab("Age of cars") + geom_hline(yintercept = 8,linetype = "dashed", color = "red") 

gridExtra::grid.arrange(caragehist,caragebox, ncol = 2)
```

Figure 3 explores the missing JOB data. The missing JOB values lines up with the population of white collar jobs. From the income, age and year on the job perspective. Also the zero inflation in income is contributed by students and home makers. The jobs above the red dashed lines the income boxplot is defined as white collar.

```{r, fig.cap="Missing JOB profile"}

cpIncome <- ggplot(data = df.rev.cleaned[!is.na(df.rev.cleaned$INCOME),], mapping = aes(x = fct_reorder(JOB,INCOME), y = INCOME,color = JOB)) + geom_boxplot() + coord_flip() + theme_bw() + xlab("JOB") + geom_vline(xintercept = 4.5,linetype = "dashed", color = "red") + theme(legend.position = "none") 

# white collar indicator variable
df.rev.cleaned <- df.rev.cleaned %>% dplyr::mutate(JOB.category = ifelse(JOB %in% c("Student", "Home Maker", "Clerical", "z_Blue_Collar"),0,1))

cpAgeblank <-  df.rev.cleaned %>% 
  filter(!is.na(INCOME)) %>% 
  filter(JOB == " ") %>% 
  select(JOB,AGE) %>% 
  mutate(JOB = ifelse(JOB == " ", "NA", JOB)) %>% 
   ggplot () + geom_density(aes(x = AGE), alpha = 0.25) + theme_bw()

cpAge.Job.cat <- df.rev.cleaned %>% 
   ggplot() + geom_density(aes(AGE, color = factor(JOB.category,labels= c("Blue color", "white color"))),alpha = 0.25) + theme_bw() + labs(color = "Jobs",cex = 0.7)
 
 cp.YOJ <- ggplot(data = df.rev.cleaned[!is.na(df.rev.cleaned$YOJ),], mapping = aes(x = fct_reorder(JOB,YOJ), y = YOJ,color = JOB)) + geom_boxplot() + coord_flip() + theme_bw() + xlab("JOB") + ylab("Year on the job") + geom_hline(yintercept = 10,linetype = "dashed", color = "red") + theme(legend.position = "none")
 
 gridExtra::grid.arrange(cpIncome,cpAgeblank, cpAge.Job.cat, cp.YOJ, ncol = 2, nrow = 2)
```

### 2.1.2 Attribute variables

The categorical variables are transformed into indicator variables to be used in modeling. The missing data points are manifested as indicator variables.


### 2.1.3 Handling missing data

The complete cases would be used to determine key features required for modeling. If data for key features are missing, an imputation strategy would be determined. 

# 3. Feature Selection

In this section we consider various feature selection methodologies such as 1. Decision Trees, 2. Peanalized model - Lasso

## 3.1 Training and Test data partition

In order to test if the feature selection are really useful, feature selections need to be cross validated on a hold out test data set. 80% of the data is used as training and the rest is used as hold out for testing. A stratified sampling method is used to capture identical percentage of samples involved in crash. 
 

```{r}
set.seed(10)
df.rev.cleaned$TARGET_FLAG2 <- ifelse(df.rev.cleaned$TARGET_FLAG==1,"Yes", "No")
trainRows <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.8, list = F)
training <- df.rev.cleaned[trainRows,]
test <- df.rev.cleaned[-trainRows,]
```


## 3.2 Decision Tree

Decision tree model is fitted on the training set to identify stand out splits in the data based on Gini index. The decision tree is shown in figure 4. Bagging technique is used to minimize variance in the model to ensure we have a reliable feature selection. The important features are shown in figure 5.

```{r, fig.cap= "Decision Tree",fig.height= 5}
# tr <- tree::tree(TARGET_FLAG2 ~. - TARGET_FLAG, data = training)
# summary(tr)
# cv.tr <- cv.tree(tr,FUN = prune.misclass,K = 100)
# library(tree)
# plot(prune.misclass(cv.tr,best = 5))
# text(prune.misclass(cv.tr,best = 5),pretty = 0)

tr2 <- rpart::rpart(TARGET_FLAG2 ~. - TARGET_FLAG, data = training,parms = list(split = 'Gini'))
#summary(tr2)
plot(tr2)
text(tr2, use.n = TRUE,cex = 0.5,pretty = 0)

```


```{r, fig.cap="Variable Importance Plot and test prediction"}
tr.bagged <- randomForest::randomForest(as.factor(TARGET_FLAG2) ~ . - TARGET_FLAG, data = training,na.action=na.omit,mtry = 28)
tr.bagged
randomForest::varImpPlot(tr.bagged, cex = 0.7, main = "Variable Importance")
abline(v=60, col= 'red',lty = 2)

library(pROC)
training$predicted.class <- predict(tr.bagged, newdata = training,type = 'response')
training$predicted.prob <- predict(tr.bagged, newdata = training, type = 'prob') [,"Yes"]
rocCurve.training <- roc(response = training$TARGET_FLAG, predictor = training$predicted.prob)
test$predicted.class <- predict(tr.bagged,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.bagged,newdata = test,type = 'prob')[,"Yes"]
#test$predicted <- ifelse(test$predicted == "Yes", 1,0)
#test$TARGET_FLAG2 <- ifelse(test$TARGET_FLAG2 == "Yes",1,0)
rocCurve.test <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)

```


The decision tree was used to predict the hold out test data and the AUC was found to be `r paste(round(pROC::auc(rocCurve.test),2)*100,"%")` as shown in figure 6. Therefore we'll use the top 13 predictors as shown in figure 4. In the next section we will explore penalized models to gather important predictors.

```{r, fig.cap="Prediction of bagged decision tree" }

plot(rocCurve.test, legacy.axes = T,asp = NA, col = 'red', main = "ROC of hold out test set")
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)),col = 'blue')
# plot(rocCurve.training, legacy.axes = T, asp = NA, add = T)
# text(0.2,0.7,paste("AUC.train:",round(pROC::auc(rocCurve.training),2)))

```

## 3.3 Penalized model - Lasso

The variable selection property of Lasso is used to aid with automated variable selection. Again the model is trained on training data set and tested on a hold out dataset, as in the decision tree method. However a 10 fold cross validation method was used to identify the optimal penalization parameter - lambda. Figure 7 shows the coefficients that are not zero in the decreasing order of absolute value of coefficients. The predictors falling above the red dashed line, drawn at the elbow in figure 7 are deemed good predictors. 

```{r, fig.cap="Feature selection - Lasso",fig.height=4}
library(glmnet)
train.factorcols <- sapply(training,is.factor)
traincols <- colnames(training)[!train.factorcols]
traincols <- traincols[which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))*-1]
training.glmnet <- training[complete.cases(training),traincols]
test.glmnet <- test[complete.cases(test),traincols]

glmnet.x <- model.matrix(TARGET_FLAG ~. , data = training.glmnet)[,-1]
glmnet.y <- as.matrix(training.glmnet$TARGET_FLAG)

# cross validation to choose lambda
gplmnet.model <- cv.glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = 10^seq(10,-2,length = 100),alpha = 1)
glmnet.model <- glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = gplmnet.model$lambda.min,alpha = 1)


# feature selection
colidx <- predict(glmnet.model,  type = "coefficients", s = gplmnet.model$lambda.min)
features <- data.frame(features = rownames(colidx), coefs = colidx[1:length(colidx)]) %>% mutate(abscoef = abs(coefs))

ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(xintercept = 44.5,linetype = "dashed", color = "red") + theme(legend.position = "none") + xlab("Features") + ylab("Absolute Coefficients")
```



## 3.3.1 Logistic regression as penalized models - Lasso

While the peanalized logistic regression model is used for feature selection, it may be used for prediction as well. All the predictors corresponding to non zero coefficients are considered as predictors. The ROC curves for the training and test samples are shown in figure 8 and the Gain chart is shown in figure 9.

```{r, fig.cap='Lasso ROC curves for test and training data',fig.height=4}
glmnet.x.test <- model.matrix(TARGET_FLAG ~. , data = test.glmnet)[,-1]
glmnet.y.test= test.glmnet$TARGET_FLAG
glmnet.test <- predict(glmnet.model, newx = glmnet.x.test, family = 'binomial',type = 'response')
rocCurve.lasso.test <- roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)


# ROC Curves for Train and Test samples
train.prob <- predict(glmnet.model,type = "response",s = gplmnet.model$lambda.min,newx = glmnet.x)
train.class <- ifelse(train.prob >= 0.5, 1, 0)
rocCurve.lasso.train <- roc(response = glmnet.y, predictor = predict(glmnet.model,type = "response",s = gplmnet.model$lambda.min,newx = glmnet.x))

# Overlaying ROC curves for train and test samples
lasso.ks.train <- round(ks.test(train.prob, glmnet.y)$statistic,2)
lasso.ks.test <- round(ks.test(glmnet.test, glmnet.y.test)$statistic,2)
plot(rocCurve.lasso.test,asp = NA,legacy.axes = T, col = 'red')
text(0.2,0.7,paste("Test AUC:", round(pROC::auc(rocCurve.lasso.test),2), "Test KS:",lasso.ks.test),col = 'red')
plot(rocCurve.lasso.train,asp = NA,legacy.axes = T, col = 'blue',add = T)
text(0.2,0.6,paste("Train AUC:", round(pROC::auc(rocCurve.lasso.train),2),"Train KS:",lasso.ks.train),col = 'blue')
title("ROC - Penalized Logistics regression")
```


```{r, "Gain chart - Lasso logistic regression"}

lasso.pred.train <- ROCR::prediction(predictions = train.prob,glmnet.y)
lasso.gain.train <- ROCR::performance(lasso.pred.train,"tpr","rpp")

lasso.pred.test <- ROCR::prediction(prediction = glmnet.test,glmnet.y.test )
lasso.gain.test <- ROCR::performance(lasso.pred.test,"tpr","rpp")

lasso.gain.train.x <- unlist(slot(lasso.gain.train,'x.values'))
lasso.gain.train.y <- unlist(slot(lasso.gain.train,'y.values'))

plot(lasso.gain.train.x,lasso.gain.train.y,lwd =2)
lines(x=c(0, 1), y=c(0, 1), type="l", col="red", lwd=2,
      ylab="True Positive Rate", 
     xlab="Rate of Positive Predictions")
lasso.gain.test.x <- unlist(slot(lasso.gain.test,'x.values'))
lasso.gain.test.y <- unlist(slot(lasso.gain.test,'y.values'))
lines(x = lasso.gain.test.x, y = lasso.gain.test.y,type = 'l',lwd = 2, col = 'orange')
lines(x=c(0, 0.35, 1), y=c(0, 1, 1), col="darkgreen", lwd=2)
legend("bottomright", col = c("black","orange","red","darkgreen"),legend = c("Training","Test","Random","ideal" ),lty = 1,lwd = 2,cex = 0.7)
title("Gain Chart - Penalized Logistics regression",cex = 0.4)

```

It can be seen from figure 8 and 9, lasso regression has 74% AUC and is better than a random model. The test performance is identical to the training performance. The misclassification is about 23%.  However performance on the records with missing values is yet evaluated and will be discussed in the modeling section. 

```{r}
# Confusion matrix train and test

predict.train.class <- ifelse(train.prob >= 0.5 ,1, 0) # Did not use Optimal cut off
confusionMatrix.train <- caret::confusionMatrix(data = predict.train.class,glmnet.y, positive = "1")
confusionMatrix.train$table
knitr::kable(round(confusionMatrix.train$overall,2), caption = "confusion matrix statistics - lasso logistic train")

predict.test.class <- as.factor(ifelse(glmnet.test >= 0.5 ,1, 0)) # Did not use Optimal cut off
confusionMatrix.test <- caret::confusionMatrix(data = predict.test.class,glmnet.y.test, positive = "1")
confusionMatrix.test$table
knitr::kable(round(confusionMatrix.test$overall,2), caption = "confusion matrix statistics - lasso logistic test")

```

## 3.4. Features selection conclusion.

The following predictors are deemed useful for modeling purposes after the above feature selection process.

```{r}
Pred.DecisionTrees <- rownames(tr.bagged$importance)[order(tr.bagged$importance,decreasing = T)][1:12]
Pred.Lasso <- as.character(features$features[order(features$abscoef, decreasing = T)] [ 1:10])
dffeatures <- matrix(c(unique(c(Pred.DecisionTrees,Pred.Lasso))[c(-14,-16)]," ", " "),ncol = 3,byrow = T)
#dffeatures <- dffeatures[which(dffeatures == '"(Intercept)"'*-1)]
# dffeatures <- dffeatures [-14,]
# dffeatures <- matrix(dffeatures, ncol = 3)
knitr::kable(dffeatures, caption = "Selected Features from Decision Tree and Lasso regression")
```

# 4. Modeling

## 4.1. Logistic regression - using all features selected in section 3


```{r}
# We'll use the training and test sets that were partitioned for the lasso

dffeatures <- dffeatures[grep('JOB', dffeatures)*-1]
jobsvars <- colnames(df.rev.cleaned)[grep('JOB', colnames(df.rev.cleaned))[-1]]
training.logistic <- training[complete.cases(training),c(dffeatures,"TARGET_FLAG",jobsvars)[c(-13,-20)]]
#write.csv(training.logistic, file = 'traininglogistic.csv')
test.logistic <- test[complete.cases(test),c(dffeatures,"TARGET_FLAG",jobsvars)[c(-13,-20)]]
train.logistic.full <- glm(TARGET_FLAG ~ . , family = binomial,data = training.logistic)
```

A logistic regression with the above selected predcitors (section 3.4) is fitted. Indicator variables for JOB is used for modeling. The summary of the fit is shown below. It can be seen from the Chi-squared goodness of fit that the model is adequate. There are some predictors whose regression coefficients are not significantly different from 0. In the next section a model without predictors with regression coeffiecients that are not significantly different from 0 would be explored. 

The model has an AUC of 81% and KS statistic whose p value is zero, indicating an adequate fit. The model has misclassification of 20% for both the training and the test samples.

The model is very identical to the Lasso regression. Choosing the variables the top 10 features from the Lasso variable selection along with features from the decision tree does not yield a model that's different from the Lasso itself. 

```{r, fig.cap="ROC curve for Logistic regression - full chosen predictors"}

summary(train.logistic.full)
anova(train.logistic.full, test = 'Chisq')

paste("Goodness of fit test... P-Value" ,1 - pchisq(train.logistic.full$null.deviance - deviance(train.logistic.full),5161-5135))

predicted.logistic.prob <- predict(object = train.logistic.full,newdata = test.logistic,type = 'response')

# plotting ROC curve
rocCurve.train.logis <- pROC::roc(response = training.logistic$TARGET_FLAG, predictor = train.logistic.full$fitted.values)
rocCurve.test.logis <- pROC::roc(response = test.logistic$TARGET_FLAG, predictor = predicted.logistic.prob)
ks.train.logis <- round(ks.test(train.logistic.full$fitted.values,training.logistic$TARGET_FLAG)$statistic,2)
ks.test.logis <- round(ks.test(predicted.logistic.prob,test.logistic$TARGET_FLAG)$statistic,2)
auc.train.logis <- round(pROC::auc(rocCurve.train.logis),2)
auc.test.logis <- round(pROC::auc(rocCurve.test.logis),2)
plot(rocCurve.train.logis,lwd = 2,col ='blue',asp = NA, legacy.axes = T)
plot(rocCurve.test.logis,lwd=2,col = 'red',add = T, asp = NA, legacy.axes = T)
legend("bottomright", col = c("blue","red"),lty = 1,legend = c("Train", "Test"),cex = 0.7)
text(x = 0.2,y = 0.6, paste("AUC:",auc.train.logis,"KS:",ks.train.logis), col = "blue")
text(x= 0.2, y= 0.5, paste("AUC:",auc.test.logis,"KS:",ks.test.logis), col = "red")
title("ROC Curve for Logistic regression")
# class prediction
predict.train.class.logis <- ifelse(train.logistic.full$fitted.values >= 0.5,1,0)
predict.test.class.logis <- ifelse(predicted.logistic.prob >= 0.5,1,0)
confusionMatrix.train.logis <- caret::confusionMatrix(data = predict.train.class.logis,training.logistic$TARGET_FLAG, positive = "1")

confusionMatrix.test.logis <- caret::confusionMatrix(data = predict.test.class.logis,test.logistic$TARGET_FLAG, positive = "1")
# Printing confusion matrix
confusionMatrix.train.logis$table
knitr::kable(round(confusionMatrix.train.logis$overall,2), caption = "Confusion matrix statistics for Losgistic regression - Training sample")
confusionMatrix.test.logis$table
knitr::kable(round(confusionMatrix.test.logis$overall,2), caption = "Confusion matric statistics for Logistic regression - Test sample " )
```


```{r, fig.cap='Gain Chart for All models so far'}

# Gain charts
logistic.pred.train <- ROCR::prediction(predictions = train.logistic.full$fitted.values,training.logistic$TARGET_FLAG)
logistic.gain.train <- ROCR::performance(logistic.pred.train,"tpr","rpp")
logistic.gain.train.x <- unlist(slot(logistic.gain.train,'x.values'))
logistic.gain.train.y <- unlist(slot(logistic.gain.train,'y.values'))
logistic.pred.test <- ROCR::prediction(prediction = predicted.logistic.prob,test.logistic$TARGET_FLAG )
logistic.gain.test <- ROCR::performance(logistic.pred.test ,"tpr","rpp")
logistic.gain.test.x <- unlist(slot(logistic.gain.test,'x.values'))
logistic.gain.test.y <- unlist(slot(logistic.gain.test,'y.values'))

plot(lasso.gain.train.x,lasso.gain.train.y,lwd =2)
lines(x=c(0, 1), y=c(0, 1), type="l", col="red", lwd=2,
      ylab="True Positive Rate", 
     xlab="Rate of Positive Predictions")
lasso.gain.test.x <- unlist(slot(lasso.gain.test,'x.values'))
lasso.gain.test.y <- unlist(slot(lasso.gain.test,'y.values'))
lines(x = lasso.gain.test.x, y = lasso.gain.test.y,type = 'l',lwd = 2, col = 'orange')
lines(x=c(0, 0.35, 1), y=c(0, 1, 1), col="darkgreen", lwd=2)
lines(x = logistic.gain.train.x,y = logistic.gain.train.y,type ='l',lty = 2, col = "blue" )
lines(x = logistic.gain.test.x,y = logistic.gain.test.y, type = 'l',lwd = 2, col = "darkblue")
legend("bottomright", col = c("black","orange","red","darkgreen", "blue", "darkblue"),legend = c("Training.Lasso","Test.Lasso","Random","ideal","Train.Logistic","Test.Logistic" ),lty = 1,lwd = 2,cex = 0.7)
title("Gain Chart - Lasso logistic and logistic",cex = 0.4)
```

### 4.1.1 Model performance on data including the missing values.

The performance of the model on complete cases seem to be good and identical to the Lasso logistic regression model. However, the model does require features that has data missing. We'll use the KNN imputation method to impute data.

```{r, fig.cap="Model performance on imputed test data"}
df.rev.imputed <- VIM::kNN(df.rev.cleaned)
set.seed(10)
trainRows.imputed <- caret::createDataPartition(df.rev.imputed$TARGET_FLAG,p = 0.8, list = F)
training.imputed <- df.rev.cleaned[trainRows.imputed,]
test.imputed <- df.rev.imputed[-trainRows.imputed,]

predicted.Imp.logistic.prob <- predict(object = train.logistic.full, newdata = test.imputed,type = 'response')
predicted.Imp.logistic.class <- ifelse(predicted.Imp.logistic.prob >= 0.5,1,0)
rocCurve.Imp.test.logistic <- pROC::roc(response = test.imputed$TARGET_FLAG, predictor = predicted.Imp.logistic.prob)
plot(rocCurve.Imp.test.logistic, asp = NA, legacy.axes = T, main = "ROC curve - Logistic regression on samples with imputed data", cex = 0.6)
legend("bottomright",lty = 1,lwd = 2, legend = "Test sample with imputed data",cex = 0.7)
text(0.1,0.7,paste("AUC:",round(pROC::auc(rocCurve.Imp.test.logistic),2)))

confusionMatrix.imp.test <- caret::confusionMatrix(data = predicted.Imp.logistic.class,test.imputed$TARGET_FLAG, positive = "1")
confusionMatrix.imp.test$table
knitr::kable(confusionMatrix.imp.test$overall, caption = "Confusion matrix stats for Logistic regression on Imputed test data")
```

The model performance is identical to what was seen in above sections.

## 4.2 Lositic regression with non-significant predictors (section 4.1) removed

```{r}
coef <- summary(train.logistic.full)$coefficients

```


## 4.3 Logistic regression - using features selected using decision tree alone.

### 4.3.1 Indicator variables

As seen in figure 4, customers with house prices less than $66680 and claims greater 974.5 are more likely to be in a crash. It'll be useful to create indicator variables denoting this for modeling. Also the JOBs are categorized as blue collar and white collar. White collar being indicated as 1 and blue collar as 0.

```{r}
df.rev.cleaned <- df.rev.cleaned %>% 
                    mutate(HOME_VAL_Bucket = ifelse(HOME_VAL < 66680, 1,0)) %>% 
                    mutate(OLDCLAIM_Bucket = ifelse(OLDCLAIM > 974.5,1,0))

df.rev.imputed <- df.rev.imputed %>% 
                    mutate(HOME_VAL_Bucket = ifelse(HOME_VAL < 66680, 1,0)) %>% 
                    mutate(OLDCLAIM_Bucket = ifelse(OLDCLAIM > 974.5,1,0))

```

### 4.3.2 Model results

It is seen from the model results that the goodness of fit is good. Also the signs of coefficients makes theoretical sense. So does models discussed above. 


```{r, fig.cap="ROC Logistic regression- predictors from Decision Tree"}


PotentialPred <- rownames(tr.bagged$importance)[order(tr.bagged$importance,decreasing = T)][1:12] 
PotentialPred <- c(PotentialPred[grep("JOB",PotentialPred)*-1],colnames(df.rev.cleaned)[grep("_Bucket", colnames(df.rev.cleaned))],colnames(df.rev.cleaned)[grep("JOB.c",colnames(df.rev.cleaned))])[c(-3,-6)]



set.seed(10)

trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG,p = 0.75, list = F)
training.dt <- df.rev.cleaned[trainRows.models,c(PotentialPred,"TARGET_FLAG")]
test.dt <- df.rev.cleaned[-trainRows.models,c(PotentialPred,"TARGET_FLAG")]

logistic.dt.train <- glm(TARGET_FLAG ~ ., family = binomial,data = training.dt)
summary(logistic.dt.train)
paste("Goodness of fit test p Value :", 1- pchisq(logistic.dt.train$null.deviance - deviance(logistic.dt.train),12))

# predict probabilities

logistic.dt.test <- predict(object = logistic.dt.train, newdata = test.dt,type = 'response')

# ROC curves
rocCurve.dt.train.logis <- pROC::roc(response = training.dt[complete.cases(training.dt),"TARGET_FLAG"], predictor = logistic.dt.train$fitted.values)
rocCurve.dt.test.logis <- pROC::roc(response = test.dt$TARGET_FLAG, predictor = logistic.dt.test)
ks.dt.test.logis <- round(ks.test(logistic.dt.test,test.dt$TARGET_FLAG)$statistic,2)
ks.dt.train.logis <- round(ks.test(logistic.dt.train$fitted.values,training.dt[complete.cases(training.dt),"TARGET_FLAG"])$statistic,2)
plot(rocCurve.dt.train.logis,asp = NA, legacy.axes = T,col = "blue")
plot(rocCurve.dt.test.logis, asp = NA, legacy.axes = T, col = "red", add = T)
text(0.2,0.6,paste("Training AUC:",round(pROC::auc(rocCurve.dt.train.logis),2), "KS:",ks.dt.train.logis),cex = 0.7)
text(0.2,0.5,paste("Test AUC:",round(pROC::auc(rocCurve.dt.test.logis),2),"KS:",ks.dt.test.logis),cex = 0.7)
legend("bottomright",lty = 1, legend = c("Train", "Test"), col = c("Blue","Red"))
title("ROC Logistic regression - predictors chosen from decison trees", cex = 0.8)

predicted.train.dt.logis.class <- ifelse(logistic.dt.train$fitted.values >= 0.5,1,0)
confusionMatrix.dt.train <- caret::confusionMatrix(data = predicted.train.dt.logis.class,training.dt[complete.cases(training.dt),"TARGET_FLAG"], positive = "1")
confusionMatrix.dt.train$table
knitr::kable(confusionMatrix.dt.train$overall, caption = "Confusion matrix stats Training set")

predicted.test.dt.logis.class <- ifelse(logistic.dt.test >= 0.5,1,0)
confusionMatrix.dt.test <- caret::confusionMatrix(data = predicted.test.dt.logis.class,test.dt$TARGET_FLAG, positive = "1")
confusionMatrix.dt.test$table
knitr::kable(confusionMatrix.dt.test$overall, caption = "Confusion matrix statsTest set")

```

```{r, eval=FALSE, include=FALSE}
x2 <- as.matrix(training.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")])
x2.test <- test.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")]
dfit2 <- data.frame(y,x2)
# write to file to check with JMP
write.csv(dfit2, file = "dfit2.csv")
fit2 <- glm(y~.,family = binomial,data = dfit2)

rocCurve.fit2 <- roc(response = dfit2[complete.cases(dfit2),'y'], predictor = fit2$fitted.values)
plot(rocCurve.fit2,asp = NA)
pROC::auc(rocCurve.fit2)
summary(fit2)

rocCurve.fit2.train <- roc(response = dfit2[complete.cases(dfit2),'y'], predictor = predictedClass2.train)
plot(rocCurve.fit2.train)
tab <- table(predictedClass2,test.models$TARGET_FLAG)
(nrow(x2.test) - sum(diag(tab))) / nrow(x2.test)
plot(rocCurve.fit2, legacy.axes = T)
pROC::auc(rocCurve.fit2)
```

