---
title: "Auto Insurance prediction"
author: "Sri Seshadri"
date: "10/20/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```

# 1. Introduction

An insurance company is interested in predicting which customers are likely to be in an accident and what would be the likely payout. The company requires this prediction to price the insurance policy. A predictive model is required to be deployed at point of request for quote or sale. The insurance company has been collecting data on which a predictive model would be trained and tested. 

## 1.1 Analysis Process

The following process steps were used for building a predicitve models:

* Exploratory Data Analysis
    + Perform data quality checks, quantify missing data.
      - Check for systemic loss in data
    + Understand relationships amongst predictors and between target variables and predictors.
      - Create attribure or indicator variables to aid data cleaning.
      - Filter out clean data for feature selection and model building.
* Feature Selection
    + Subset complete records to model wins in season
      - Use different modeling techniques to select candidate predictors.
      - If data is missing for candidate predictors, identify imputing methods.
* Model Building
    + Test models that were build using complete records on the entire data set with imputed data.
    + Compare models based on Deviance, ROC and MAE
      - Check if models make physical sense. 
* Initial model deployment
    + Deploy model to predict wins on out of sample data.
    + Discuss models and results with subject matter experts.
    + Fine tune model and re-test
* Final model deployment
  
    
## 1.2 Executive summary


# 2. Data

The insurance comnpany has data collected from almost 8200 customers. The dictionary of the data is provided in the appendix A.1. Tables 1 and 2 show the summary statistics of numeric and non-numeric features of the data. It is seen that some features have missing values. The missing values may need to be imputed if the features are deemed important predictors of likelihood of customer involving in a crash or payout. 

It is seen that the minimum age of car is -3. Which is not rational, the data is filled in with +3 assuming it is a typographical error. Also it is seen that there is white space in the JOB column of the data. 

```{r Import}
# import data
df <- read.csv('logit_insurance_rev.csv')

# Sanity Check of numeric variables
library(mosaic)
sanitycheck <- do.call(rbind,dfapply(df,favstats, select = is.numeric))
knitr::kable(round(sanitycheck,2), caption = "Summary statistics")

# Sanoty check on non-numeric variables
sanitycheckcharacter <-select(df, colnames(df[1,sapply(df,class) == 'factor']))
library(purrr)
UniqueVals <- sanitycheckcharacter %>% 
  map(unique)

Counts <- data.frame(sapply(UniqueVals,length),
                     do.call(rbind,dfapply(df,length,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,n_missing,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,function(x)length(x[which(x == " ")]))),
                     row.names = names(UniqueVals))
colnames(Counts) <- c( "# Unique", "n","missing","Blanks")
knitr::kable(Counts, caption = 'Sanity check of non numeric variables')

# Creating attribute columns for missing variables
df.rev <- df %>% 
            dplyr::mutate(YOJ_Missing = ifelse(is.na(YOJ),1,0)) %>% 
            dplyr::mutate(Income_Missing = ifelse(is.na(INCOME),1,0)) %>% 
            dplyr::mutate(HOME_Val_Missing = ifelse(is.na(HOME_VAL),1,0)) %>% 
            dplyr::mutate(CAR_AGE_Missing = ifelse(is.na(CAR_AGE),1,0)) %>% 
            dplyr::mutate(AGE_Missing = ifelse(is.na(AGE),1,0)) %>% 
            dplyr::mutate(CAR_AGE = ifelse(!is.na(CAR_AGE) & CAR_AGE <0, abs(CAR_AGE), CAR_AGE))

# save file for external data analysis
# write.table(df.rev,'insurancedata.csv')

# Defining predictors and temporarily removing Target amount from the data
predictors <- colnames(df.rev)[which(colnames(df.rev) %in% c("INDEX", "TARGET_AMT"))*-1]
df.rev.cleaned <- df.rev %>% 
                    dplyr::select(c('TARGET_FLAG',predictors)) 
write.csv(df.rev.cleaned, file = 'dfrevcleaned.csv')
# creating indicator variables for factor variables

factorcols <- sapply(df.rev.cleaned,is.factor)
dummyvars <- dummy::dummy(df.rev.cleaned[,factorcols],int = T)
df.rev.cleaned <- cbind(df.rev.cleaned,dummyvars)
#df.rev.cleaned.comp <- df.rev.cleaned[complete.cases(df.rev.cleaned), c(- 1,-7,-23:-27)]          
```
## 2.1 Exploratory Data Analysis (EDA)

```{r}
library(forcats)
library(ggplot2)
ggplot(data = df.rev.cleaned[!is.na(df.rev.cleaned$INCOME),], mapping = aes(x = fct_reorder(JOB,INCOME), y = INCOME,color = JOB)) + geom_boxplot() + coord_flip() + theme_bw() + xlab("JOB") + geom_vline(xintercept = 4.5,linetype = "dashed", color = "red") + theme(legend.position = "none")

df.rev.cleaned <- df.rev.cleaned %>% dplyr::mutate(JOB.category = ifelse(JOB %in% c("Student", "Home Maker", "Clerical", "z_Blue_Collar"),0,1))


corcols <- sapply(df.rev.cleaned,is.numeric)
corcols[grep("_Missing",colnames(df.rev.cleaned))] <- F
cordata <- cor(df.rev.cleaned[complete.cases(df.rev.cleaned), corcols])
corrplot::corrplot(cordata,tl.cex = 0.6)
```



# 3. Feature Selection

In this section we consider various feature selection methodologies such as 1. Decision Trees, 2. Peanalized model - Lasso 

## 3.1 Training and Test data partition

In order to test if the feature selection are really useful, feature selections need to be cross validated on a hold out test data set. 80% of the data is used as training and the rest is used as hold out for testing.
 

```{r}
set.seed(10)
df.rev.cleaned$TARGET_FLAG2 <- ifelse(df.rev.cleaned$TARGET_FLAG==1,"Yes", "No")
trainRows <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.8, list = F)
training <- df.rev.cleaned[trainRows,]
test <- df.rev.cleaned[-trainRows,]
```


## 3.2 Decision Tree

Decision tree model is fitted on the training set to identify stand out splits in the data based on Gini index. The decision tree is shown in figure 3. Bagging technique is used to minimize variance in the model to ensure we have a reliable feature selection. The important features are shown in figure 4.

```{r, fig.cap= "Decision Tree"}
# tr <- tree::tree(TARGET_FLAG2 ~. - TARGET_FLAG, data = training)
# summary(tr)
# cv.tr <- cv.tree(tr,FUN = prune.misclass,K = 100)
# library(tree)
# plot(prune.misclass(cv.tr,best = 5))
# text(prune.misclass(cv.tr,best = 5),pretty = 0)

tr2 <- rpart::rpart(TARGET_FLAG2 ~. - TARGET_FLAG, data = training,parms = list(split = 'Gini'))
#summary(tr2)
plot(tr2)
text(tr2, use.n = TRUE,cex = 0.5,pretty = 0)

```


```{r, fig.cap="Variable Importance Plot and test prediction"}
tr.bagged <- randomForest::randomForest(as.factor(TARGET_FLAG2) ~ . - TARGET_FLAG, data = training,na.action=na.omit,mtry = 28)
tr.bagged
randomForest::varImpPlot(tr.bagged, cex = 0.7, main = "Variable Importance")
abline(v=60, col= 'red',lty = 2)

library(pROC)
training$predicted.class <- predict(tr.bagged, newdata = training,type = 'response')
training$predicted.prob <- predict(tr.bagged, newdata = training, type = 'prob') [,"Yes"]
rocCurve.training <- roc(response = training$TARGET_FLAG, predictor = training$predicted.prob)
test$predicted.class <- predict(tr.bagged,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.bagged,newdata = test,type = 'prob')[,"Yes"]
#test$predicted <- ifelse(test$predicted == "Yes", 1,0)
#test$TARGET_FLAG2 <- ifelse(test$TARGET_FLAG2 == "Yes",1,0)
rocCurve.test <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)

```
The decision tree was used to predict the hold out test data and the AUC was found to be `r paste(round(pROC::auc(rocCurve.test),2)*100,"%")` as shown in figure 5. Therefore we'll use the top 13 predictors as shown in figure 4. In the next section we will explore penalized models to gather important predictors.

```{r, fig.cap="Prediction of bagged decision tree" }

plot(rocCurve.test, legacy.axes = T,asp = NA, col = 'red', main = "ROC of hold out test set")
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)),col = 'blue')
# plot(rocCurve.training, legacy.axes = T, asp = NA, add = T)
# text(0.2,0.7,paste("AUC.train:",round(pROC::auc(rocCurve.training),2)))

```

## 3.3 Penalized model - Lasso



```{r}
library(glmnet)
train.factorcols <- sapply(training,is.factor)
traincols <- colnames(training)[!train.factorcols]
traincols <- traincols[which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))*-1]
training.glmnet <- training[complete.cases(training),traincols]
test.glmnet <- test[complete.cases(test),traincols]

glmnet.x <- model.matrix(TARGET_FLAG ~. , data = training.glmnet)[,-1]
glmnet.y <- as.matrix(training.glmnet$TARGET_FLAG)

# cross validation to choose lambda
gplmnet.model <- cv.glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = 10^seq(10,-2,length = 100))
glmnet.model <- glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = gplmnet.model$lambda.min)

# feature selection
colidx <- predict(glmnet.model,  type = "coefficients", s = gplmnet.model$lambda.min)
features <- data.frame(features = rownames(colidx), coefs = colidx[1:length(colidx)]) %>% mutate(abscoef = abs(coefs))
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(xintercept = 44.5,linetype = "dashed", color = "red") + theme(legend.position = "none") + xlab("Features") + ylab("Absolute Coefficients")

glmnet.x.test <- model.matrix(TARGET_FLAG ~. , data = test.glmnet)[,-1]
glmnet.y.test= test.glmnet$TARGET_FLAG
glmnet.test <- predict(glmnet.model, newx = glmnet.x.test, response = 'prob')
rocCurve.lasso.train <- roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)
plot(rocCurve.lasso.train,asp = NA,legacy.axes = T, col = 'red')
text(0.2,0.7,paste("AUC:", round(pROC::auc(rocCurve.lasso),2)),col = 'blue')
```


```{r}

summary(training)
PotentialPred <- rownames(tr.bagged$importance)[order(tr.bagged$importance,decreasing = T)][1:12]
#df.rev.imputed <- VIM::kNN(df.rev.cleaned)

#

trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
training.models <- df.rev.cleaned[trainRows.models,]
test.models <- df.rev.cleaned[-trainRows.models,]
y <- ifelse(training.models$TARGET_FLAG2 == "No",0,1)
x <- as.matrix(training.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])])
dffit <- data.frame(y,x)
fit1 <- glm(y~.,family = binomial,data = dffit[complete.cases(dffit),])

summary(fit1)
rocCurve.fit1.train <- roc(response = dffit[complete.cases(dffit),"y"], predictor = fit1$fitted.values)
plot(rocCurve.fit1.train,legacy.axes = T, asp = NA)
pROC::auc(rocCurve.fit1.train)

x.test <- test.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])]
predictedClass <- ifelse(predict.glm(fit1,newdata = x.test, type = 'response') >= 0.5, 1,0)

test.y <- ifelse(test.models$TARGET_FLAG2 == "No",0,1)
table(predictedClass,test.y)
rocCurve.fit1 <- roc(response = test.y, predictor = predict.glm(fit1,newdata = x.test, type = 'response'))
plot(rocCurve.fit1, legacy.axes = T)
pROC::auc(rocCurve.fit1)
```

```{r}
x2 <- as.matrix(training.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")])
x2.test <- test.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")]
dfit2 <- data.frame(y,x2)
# write to file to check with JMP
write.csv(dfit2, file = "dfit2.csv")
fit2 <- glm(y~.,family = binomial,data = dfit2)

rocCurve.fit2 <- roc(response = dfit2[complete.cases(dfit2),'y'], predictor = fit2$fitted.values)
plot(rocCurve.fit2)
auc(rocCurve.fit2)
summary(fit2)

rocCurve.fit2.train <- roc(response = dfit2[complete.cases(dfit2),'y'], predictor = predictedClass2.train)
plot(rocCurve.fit2.train)
tab <- table(predictedClass2,test.models$TARGET_FLAG)
(nrow(x2.test) - sum(diag(tab))) / nrow(x2.test)
plot(rocCurve.fit2, legacy.axes = T)
pROC::auc(rocCurve.fit2)
```

