simulatedTest <- quadBoundaryFunc(1000)
library(randomForest)
rfModel <- rando,randomForest(class ~ X1 + X2, data = simulatedTrain, ntree = 200)
rfModel <- randomForest(class ~ X1 + X2, data = simulatedTrain, ntree = 200)
plot(rfModel)
knitr::opts_chunk$set(echo = F, warning = F, message = F)
# import data
df <- read.csv('logit_insurance_rev.csv')
# Sanity Check of numeric variables
library(mosaic)
sanitycheck <- do.call(rbind,dfapply(df,favstats, select = is.numeric))
knitr::kable(round(sanitycheck,2), caption = "Summary statistics")
# Sanoty check on non-numeric variables
sanitycheckcharacter <-select(df, colnames(df[1,sapply(df,class) == 'factor']))
library(purrr)
UniqueVals <- sanitycheckcharacter %>%
map(unique)
Counts <- data.frame(sapply(UniqueVals,length),
do.call(rbind,dfapply(df,length,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,n_missing,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,function(x)length(x[which(x == " ")]))),
row.names = names(UniqueVals))
colnames(Counts) <- c( "# Unique", "n","missing","Blanks")
knitr::kable(Counts, caption = 'Sanity check of non numeric variables')
# Creating attribute columns for missing variables
df.rev <- df %>%
dplyr::mutate(YOJ_Missing = ifelse(is.na(YOJ),1,0)) %>%
dplyr::mutate(Income_Missing = ifelse(is.na(INCOME),1,0)) %>%
dplyr::mutate(HOME_Val_Missing = ifelse(is.na(HOME_VAL),1,0)) %>%
dplyr::mutate(CAR_AGE_Missing = ifelse(is.na(CAR_AGE),1,0)) %>%
dplyr::mutate(AGE_Missing = ifelse(is.na(AGE),1,0)) %>%
dplyr::mutate(CAR_AGE = ifelse(!is.na(CAR_AGE) & CAR_AGE <0, abs(CAR_AGE), CAR_AGE))
# save file for external data analysis
# write.table(df.rev,'insurancedata.csv')
# Defining predictors and temporarily removing Target amount from the data
predictors <- colnames(df.rev)[which(colnames(df.rev) %in% c("INDEX", "TARGET_AMT"))*-1]
df.rev.cleaned <- df.rev %>%
dplyr::select(c('TARGET_FLAG',predictors))
write.csv(df.rev.cleaned, file = 'dfrevcleaned.csv')
# creating indicator variables for factor variables
factorcols <- sapply(df.rev.cleaned,is.factor)
dummyvars <- dummy::dummy(df.rev.cleaned[,factorcols],int = T)
df.rev.cleaned <- cbind(df.rev.cleaned,dummyvars)
#df.rev.cleaned.comp <- df.rev.cleaned[complete.cases(df.rev.cleaned), c(- 1,-7,-23:-27)]
library(forcats)
library(ggplot2)
ggplot(data = df.rev.cleaned[!is.na(df.rev.cleaned$INCOME),], mapping = aes(x = fct_reorder(JOB,INCOME), y = INCOME,color = JOB)) + geom_boxplot() + coord_flip() + theme_bw() + xlab("JOB") + geom_vline(xintercept = 4.5,linetype = "dashed", color = "red") + theme(legend.position = "none")
df.rev.cleaned <- df.rev.cleaned %>% dplyr::mutate(JOB.category = ifelse(JOB %in% c("Student", "Home Maker", "Clerical", "z_Blue_Collar"),0,1))
corcols <- sapply(df.rev.cleaned,is.numeric)
corcols[grep("_Missing",colnames(df.rev.cleaned))] <- F
cordata <- cor(df.rev.cleaned[complete.cases(df.rev.cleaned), corcols])
corrplot::corrplot(cordata,tl.cex = 0.6)
set.seed(10)
df.rev.cleaned$TARGET_FLAG2 <- ifelse(df.rev.cleaned$TARGET_FLAG==1,"Yes", "No")
trainRows <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.8, list = F)
training <- df.rev.cleaned[trainRows,]
test <- df.rev.cleaned[-trainRows,]
tr2 <- rpart::rpart(TARGET_FLAG2 ~. - TARGET_FLAG, data = training,parms = list(split = 'AUC'))
tr2 <- rpart::rpart(TARGET_FLAG2 ~. - TARGET_FLAG, data = training,parms = list(split = 'AUC'))
summary(tr2)
plot(tr2)
text(tr2, use.n = TRUE,cex = 0.5,pretty = 0)
tr.boosted <- randomForest::randomForest(as.factor(TARGET_FLAG2) ~ . - TARGET_FLAG, data = training,na.action=na.omit,mtry = 28)
plot(tr.boosted)
rftest <- predict(object = rfModel, simulatedTest, type = "response")
rftest
rftest.class <- predict(object = rfModel, simulatedTest, type = "response")
rftest.prob <- predict(object = rfModel, simulatedTest, type = 'prob')
rftest.prob
??predict
test$predicted <- predict(tr.boosted,newdata = test,type = 'response')
test$predicted.class <- predict(tr.boosted,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.boosted,newdata = test,type = 'prob')
test$predicted.prob
head(test$predicted.prob)
test$predicted.prob <- predict(tr.boosted,newdata = test,type = 'prob')[,"Yes"]
head(test$predicted.prob)
rocCurve <- roc(response = test$predicted.class , predictor = test$predicted.prob)
library(pROC)
rocCurve <- roc(response = test$predicted.class , predictor = test$predicted.prob)
plot(rocCurve, legacy.axes = T,xlim = c(1,0), ylim = c(0,1))
pROC::auc(rocCurve)
plot(rocCurve, legacy.axes = T,xlim = c(1,0), ylim = c(0,1))
rocCurve <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)
plot(rocCurve, legacy.axes = T,xlim = c(1,0), ylim = c(0,1))
pROC::auc(rocCurve)
PotentialPred <- rownames(tr.boosted$importance)[order(tr.boosted$importance,decreasing = T)][1:12]
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
training.models <- df.rev.cleaned[trainRows.models,]
test.models <- df.rev.cleaned[-trainRows.models,]
y <- ifelse(training.models$TARGET_FLAG2 == "No",0,1)
x <- as.matrix(training.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])])
dffit <- data.frame(y,x)
fit1 <- glm(y~.,family = binomial,data = dffit[complete.cases(dffit),])
summary(fit1)
head(fit1$fitted.values)
fit1.pred <- predict(fit1,newdata = dffit, type = 'prob')
fit1.pred <- predict(fit1,newdata = dffit, type = 'link')
head(fit1.pred)
fit1.pred <- predict(fit1,newdata = dffit, type = 'terms')
rocCurve.fit1.train <- roc(response = dfit$y, predictor = fit1$fitted.values)
PotentialPred <- rownames(tr.boosted$importance)[order(tr.boosted$importance,decreasing = T)][1:12]
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
training.models <- df.rev.cleaned[trainRows.models,]
test.models <- df.rev.cleaned[-trainRows.models,]
y <- ifelse(training.models$TARGET_FLAG2 == "No",0,1)
x <- as.matrix(training.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])])
dffit <- data.frame(y,x)
fit1 <- glm(y~.,family = binomial,data = dffit[complete.cases(dffit),])
summary(fit1)
rocCurve.fit1.train <- roc(response = dfit$y, predictor = fit1$fitted.values)
rocCurve.fit1.train <- roc(response = dffit$y, predictor = fit1$fitted.values)
rocCurve.fit1.train <- roc(response = dffit[complete.cases(dffit),"y"], predictor = fit1$fitted.values)
plot(rocCurve.fit1.train)
auc(rocCurve.fit1.train)
x2 <- as.matrix(training.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")])
x2.test <- test.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")]
x.test <- test.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])]
predictedClass <- ifelse(predict.glm(fit1,newdata = x.test, type = 'response') >= 0.5, 1,0)
test.y <- ifelse(test.models$TARGET_FLAG2 == "No",0,1)
rocCurve.fit1 <- roc(response = test.y, predictor = predict.glm(fit1,newdata = x.test, type = 'response'))
plot(rocCurve.fit1, legacy.axes = T)
pROC::auc(rocCurve.fit1)
auc(rocCurve.fit1.train)
plot(rocCurve.fit1, legacy.axes = T)
x2 <- as.matrix(training.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")])
x2.test <- test.models[,c("AGE","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM","CLM_FREQ","MVR_PTS")]
dfit2 <- data.frame(y,x2)
write.csv(dfit2, file = "dfit2.csv")
fit2 <- glm(y~.,family = binomial,data = dfit2)
rocCurve.fit2 <- roc(response = test.models$TARGET_FLAG, predictor = fit2$fitted.values)
rocCurve.fit2 <- roc(response = test.models[complete.cases(test.models),TARGET_FLAG], predictor = fit2$fitted.values)
rocCurve.fit2 <- roc(response = test.models[complete.cases(test.models),'TARGET_FLAG'], predictor = fit2$fitted.values)
rocCurve.fit2 <- roc(response = dfit2$y, predictor = fit2$fitted.values)
rocCurve.fit2 <- roc(response = dfit2[complete.cases(dfit2),'y'], predictor = fit2$fitted.values)
plot(rocCurve.fit2)
auc(rocCurve.fit2)
knitr::opts_chunk$set(echo = F, warning = F, message = F)
# import data
df <- read.csv('logit_insurance_rev.csv')
# Sanity Check of numeric variables
library(mosaic)
sanitycheck <- do.call(rbind,dfapply(df,favstats, select = is.numeric))
knitr::kable(round(sanitycheck,2), caption = "Summary statistics")
# Sanoty check on non-numeric variables
sanitycheckcharacter <-select(df, colnames(df[1,sapply(df,class) == 'factor']))
library(purrr)
UniqueVals <- sanitycheckcharacter %>%
map(unique)
Counts <- data.frame(sapply(UniqueVals,length),
do.call(rbind,dfapply(df,length,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,n_missing,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,function(x)length(x[which(x == " ")]))),
row.names = names(UniqueVals))
colnames(Counts) <- c( "# Unique", "n","missing","Blanks")
knitr::kable(Counts, caption = 'Sanity check of non numeric variables')
# Creating attribute columns for missing variables
df.rev <- df %>%
dplyr::mutate(YOJ_Missing = ifelse(is.na(YOJ),1,0)) %>%
dplyr::mutate(Income_Missing = ifelse(is.na(INCOME),1,0)) %>%
dplyr::mutate(HOME_Val_Missing = ifelse(is.na(HOME_VAL),1,0)) %>%
dplyr::mutate(CAR_AGE_Missing = ifelse(is.na(CAR_AGE),1,0)) %>%
dplyr::mutate(AGE_Missing = ifelse(is.na(AGE),1,0)) %>%
dplyr::mutate(CAR_AGE = ifelse(!is.na(CAR_AGE) & CAR_AGE <0, abs(CAR_AGE), CAR_AGE))
# save file for external data analysis
# write.table(df.rev,'insurancedata.csv')
# Defining predictors and temporarily removing Target amount from the data
predictors <- colnames(df.rev)[which(colnames(df.rev) %in% c("INDEX", "TARGET_AMT"))*-1]
df.rev.cleaned <- df.rev %>%
dplyr::select(c('TARGET_FLAG',predictors))
write.csv(df.rev.cleaned, file = 'dfrevcleaned.csv')
# creating indicator variables for factor variables
factorcols <- sapply(df.rev.cleaned,is.factor)
dummyvars <- dummy::dummy(df.rev.cleaned[,factorcols],int = T)
df.rev.cleaned <- cbind(df.rev.cleaned,dummyvars)
#df.rev.cleaned.comp <- df.rev.cleaned[complete.cases(df.rev.cleaned), c(- 1,-7,-23:-27)]
library(forcats)
library(ggplot2)
ggplot(data = df.rev.cleaned[!is.na(df.rev.cleaned$INCOME),], mapping = aes(x = fct_reorder(JOB,INCOME), y = INCOME,color = JOB)) + geom_boxplot() + coord_flip() + theme_bw() + xlab("JOB") + geom_vline(xintercept = 4.5,linetype = "dashed", color = "red") + theme(legend.position = "none")
df.rev.cleaned <- df.rev.cleaned %>% dplyr::mutate(JOB.category = ifelse(JOB %in% c("Student", "Home Maker", "Clerical", "z_Blue_Collar"),0,1))
corcols <- sapply(df.rev.cleaned,is.numeric)
corcols[grep("_Missing",colnames(df.rev.cleaned))] <- F
cordata <- cor(df.rev.cleaned[complete.cases(df.rev.cleaned), corcols])
corrplot::corrplot(cordata,tl.cex = 0.6)
set.seed(10)
df.rev.cleaned$TARGET_FLAG2 <- ifelse(df.rev.cleaned$TARGET_FLAG==1,"Yes", "No")
trainRows <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.8, list = F)
training <- df.rev.cleaned[trainRows,]
test <- df.rev.cleaned[-trainRows,]
# tr <- tree::tree(TARGET_FLAG2 ~. - TARGET_FLAG, data = training)
# summary(tr)
# cv.tr <- cv.tree(tr,FUN = prune.misclass,K = 100)
# library(tree)
# plot(prune.misclass(cv.tr,best = 5))
# text(prune.misclass(cv.tr,best = 5),pretty = 0)
tr2 <- rpart::rpart(TARGET_FLAG2 ~. - TARGET_FLAG, data = training,parms = list(split = 'Gini'))
summary(tr2)
plot(tr2)
text(tr2, use.n = TRUE,cex = 0.5,pretty = 0)
tr.bagged <- randomForest::randomForest(as.factor(TARGET_FLAG2) ~ . - TARGET_FLAG, data = training,na.action=na.omit,mtry = 28)
tr.bagged
randomForest::varImpPlot(tr.boosted, cex = 0.7)
randomForest::varImpPlot(tr.bagged, cex = 0.7)
plot(tr.bagged)
tr.bagged
plot(tr2)
text(tr2, use.n = TRUE,cex = 0.5,pretty = 0)
summary(tr2)
tr.bagged <- randomForest::randomForest(as.factor(TARGET_FLAG2) ~ . - TARGET_FLAG, data = training,na.action=na.omit,mtry = 28)
tr.bagged
library(pROC)
test$predicted.class <- predict(tr.bagged,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.bagged,newdata = test,type = 'prob')[,"Yes"]
test$predicted <- ifelse(test$predicted == "Yes", 1,0)
library(pROC)
test$predicted.class <- predict(tr.bagged,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.bagged,newdata = test,type = 'prob')[,"Yes"]
#test$predicted <- ifelse(test$predicted == "Yes", 1,0)
test$TARGET_FLAG2 <- ifelse(test$TARGET_FLAG2 == "Yes",1,0)
rocCurve <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)
plot(rocCurve, legacy.axes = T,xlim = c(1,0), ylim = c(0,1), main = paste('AUC: ',pROC::auc(rocCurve) ))
plot(rocCurve, legacy.axes = T,asp = NA, text(0.8,0.8,paste("AUC" = pROC::auc(rocCurve))))
rocCurve <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)
plot(rocCurve, legacy.axes = T,asp = NA, text(0.8,0.8,paste("AUC" = pROC::auc(rocCurve))))
plot(rocCurve, legacy.axes = T,asp = NA)
text(0.8,0.8,paste("AUC" = pROC::auc(rocCurve)))
plot(rocCurve, legacy.axes = T,asp = NA)
text(0.2,0.8,paste("AUC" = pROC::auc(rocCurve)))
plot(rocCurve, legacy.axes = T,asp = NA)
text(0.2,0.8,paste("AUC = ",round(pROC::auc(rocCurve),2)))
plot(rocCurve, legacy.axes = T,asp = NA)
text(0.2,0.8,paste("AUC:",round(pROC::auc(rocCurve),2)))
pROC::auc(rocCurve)
# tr <- tree::tree(TARGET_FLAG2 ~. - TARGET_FLAG, data = training)
# summary(tr)
# cv.tr <- cv.tree(tr,FUN = prune.misclass,K = 100)
# library(tree)
# plot(prune.misclass(cv.tr,best = 5))
# text(prune.misclass(cv.tr,best = 5),pretty = 0)
tr2 <- rpart::rpart(TARGET_FLAG2 ~. - TARGET_FLAG, data = training,parms = list(split = 'Gini'))
#summary(tr2)
plot(tr2)
text(tr2, use.n = TRUE,cex = 0.5,pretty = 0)
# import data
df <- read.csv('logit_insurance_rev.csv')
# Sanity Check of numeric variables
library(mosaic)
sanitycheck <- do.call(rbind,dfapply(df,favstats, select = is.numeric))
knitr::kable(round(sanitycheck,2), caption = "Summary statistics")
# Sanoty check on non-numeric variables
sanitycheckcharacter <-select(df, colnames(df[1,sapply(df,class) == 'factor']))
library(purrr)
UniqueVals <- sanitycheckcharacter %>%
map(unique)
Counts <- data.frame(sapply(UniqueVals,length),
do.call(rbind,dfapply(df,length,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,n_missing,select = is.factor)),do.call(rbind,dfapply(sanitycheckcharacter,function(x)length(x[which(x == " ")]))),
row.names = names(UniqueVals))
colnames(Counts) <- c( "# Unique", "n","missing","Blanks")
knitr::kable(Counts, caption = 'Sanity check of non numeric variables')
# Creating attribute columns for missing variables
df.rev <- df %>%
dplyr::mutate(YOJ_Missing = ifelse(is.na(YOJ),1,0)) %>%
dplyr::mutate(Income_Missing = ifelse(is.na(INCOME),1,0)) %>%
dplyr::mutate(HOME_Val_Missing = ifelse(is.na(HOME_VAL),1,0)) %>%
dplyr::mutate(CAR_AGE_Missing = ifelse(is.na(CAR_AGE),1,0)) %>%
dplyr::mutate(AGE_Missing = ifelse(is.na(AGE),1,0)) %>%
dplyr::mutate(CAR_AGE = ifelse(!is.na(CAR_AGE) & CAR_AGE <0, abs(CAR_AGE), CAR_AGE))
# save file for external data analysis
# write.table(df.rev,'insurancedata.csv')
# Defining predictors and temporarily removing Target amount from the data
predictors <- colnames(df.rev)[which(colnames(df.rev) %in% c("INDEX", "TARGET_AMT"))*-1]
df.rev.cleaned <- df.rev %>%
dplyr::select(c('TARGET_FLAG',predictors))
write.csv(df.rev.cleaned, file = 'dfrevcleaned.csv')
# creating indicator variables for factor variables
factorcols <- sapply(df.rev.cleaned,is.factor)
dummyvars <- dummy::dummy(df.rev.cleaned[,factorcols],int = T)
df.rev.cleaned <- cbind(df.rev.cleaned,dummyvars)
#df.rev.cleaned.comp <- df.rev.cleaned[complete.cases(df.rev.cleaned), c(- 1,-7,-23:-27)]
tr.bagged <- randomForest::randomForest(as.factor(TARGET_FLAG2) ~ . - TARGET_FLAG, data = training,na.action=na.omit,mtry = 28)
tr.bagged
randomForest::varImpPlot(tr.bagged, cex = 0.7, main = "Variable Importance")
library(pROC)
training$predicted.class <- predict(tr.bagged, newdata = training,type = 'response')
training$predicted.prob <- predict(tr.bagged, newdata = training, type = 'prob') [,"Yes"]
rocCurve.training <- roc(response = training$TARGET_FLAG, predictor = training$predicted.prob)
test$predicted.class <- predict(tr.bagged,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.bagged,newdata = test,type = 'prob')[,"Yes"]
#test$predicted <- ifelse(test$predicted == "Yes", 1,0)
#test$TARGET_FLAG2 <- ifelse(test$TARGET_FLAG2 == "Yes",1,0)
rocCurve.test <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)
plot(rocCurve.test, legacy.axes = T,asp = NA)
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)))
plot(rocCurve.training, legacy.axes = T, asp = NA, add = T)
text(0.2,0.7,paste("AUC.train:",round(pROC::auc(rocCurve.training),2)))
tr.bagged$predicted
training$predicted.prob
predict(tr.bagged, newdata = training, type = 'prob')
rocCurve.training <- roc(response = training$TARGET_FLAG, predictor = training$predicted.prob)
plot(rocCurve.test, legacy.axes = T,asp = NA)
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)))
plot(rocCurve.training, legacy.axes = T, asp = NA, add = T)
text(0.2,0.7,paste("AUC.train:",round(pROC::auc(rocCurve.training),2)))
library(pROC)
training$predicted.class <- predict(tr.bagged, newdata = training,type = 'response')
training$predicted.prob <- predict(tr.bagged, newdata = training, type = 'prob') [,"Yes"]
rocCurve.training <- roc(response = training$TARGET_FLAG, predictor = training$predicted.prob)
test$predicted.class <- predict(tr.bagged,newdata = test,type = 'response')
test$predicted.prob <- predict(tr.bagged,newdata = test,type = 'prob')[,"Yes"]
#test$predicted <- ifelse(test$predicted == "Yes", 1,0)
#test$TARGET_FLAG2 <- ifelse(test$TARGET_FLAG2 == "Yes",1,0)
rocCurve.test <- roc(response = test$TARGET_FLAG , predictor = test$predicted.prob)
plot(rocCurve.test, legacy.axes = T,asp = NA, col = 'red')
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)))
# plot(rocCurve.training, legacy.axes = T, asp = NA, add = T)
# text(0.2,0.7,paste("AUC.train:",round(pROC::auc(rocCurve.training),2)))
plot(rocCurve.test, legacy.axes = T,asp = NA, col = 'red')
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)),col = 'blue')
plot(rocCurve.test, legacy.axes = T,asp = NA, col = 'red', main = "ROC of hold out test set")
text(0.2,0.8,paste("AUC.test:",round(pROC::auc(rocCurve.test),2)),col = 'blue')
randomForest::varImpPlot(tr.bagged, cex = 0.7, main = "Variable Importance")
abline(v=60)
randomForest::varImpPlot(tr.bagged, cex = 0.7, main = "Variable Importance")
abline(v=60, col= 'red',lty = 2)
factorcols
train.factorcols <- sapply(training,is.factor)
train.factorcols
colnames(training)[!train.factorcols]
traincols[!which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))]
traincols <- colnames(training)[!train.factorcols]
traincols[!which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))]
traincols
which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))
traincols[which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))*-1]
training.glmnet <- training[,traincols]
glmnet.x <- model.matrix(TARGET_FLAG ~. , data = training.glmnet)
glmnet.y <- as.matrix(training.glmnet$TARGET_FLAG)
View(glmnet.x)
glmnet.x <- model.matrix(TARGET_FLAG ~. , data = training.glmnet)[,-1]
gplmnet.model <- glmnet(x = glmnet.x, y = glmnet.y, family = 'binomial')
library(glmnet)
gplmnet.model <- glmnet(x = glmnet.x, y = glmnet.y, family = 'binomial')
View(glmnet.x)
View(glmnet.y)
dim(glmnet.x)
length(glmnet.y)
length(glmnet.y[!is.na(glmnet.y)])
length(glmnet.y[complete.cases(glmnet.y)])
training.glmnet <- training[complete.cases(training),traincols]
glmnet.x <- model.matrix(TARGET_FLAG ~. , data = training.glmnet)[,-1]
glmnet.y <- as.matrix(training.glmnet$TARGET_FLAG)
dim(glmnet.x)
length(glmnet.y)
gplmnet.model <- glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial')
glmnet.model
gplmnet.model
summary(gplmnet.model)
gplmnet.model <- cv.glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = 10^seq(10,-2,length = 100))
plot(gplmnet.model)
glmnet.model <- glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = gplmnet.model$lambda.min)
predict(glmnet.model, newx = glmnet.x, type = "nonzero")
predict(glmnet.model, newx = glmnet.x, type = "nonzero", s = gplmnet.model$lambda.min)
colnames(glmnet.x)[54]
traincols
train.factorcols <- sapply(training,is.factor)
traincols <- colnames(training)[!train.factorcols]
traincols <- traincols[which(traincols %in% c("TARGET_FLAG2", "predicted.prob"))*-1]
traincols
training.glmnet <- training[complete.cases(training),traincols]
glmnet.x <- model.matrix(TARGET_FLAG ~. , data = training.glmnet)[,-1]
glmnet.y <- as.matrix(training.glmnet$TARGET_FLAG)
gplmnet.model <- cv.glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = 10^seq(10,-2,length = 100))
glmnet.model <- glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = gplmnet.model$lambda.min)
predict(glmnet.model, newx = glmnet.x, type = "nonzero", s = gplmnet.model$lambda.min)
colnames(glmnet.x)[predict(glmnet.model, newx = glmnet.x, type = "nonzero", s = gplmnet.model$lambda.min)]
colidx <- predict(glmnet.model, newx = glmnet.x, type = "nonzero", s = gplmnet.model$lambda.min)
colnames(glmnet.x)[t(colidx)]
colidx <- predict(glmnet.model, newx = glmnet.x, s = gplmnet.model$lambda.min)
colidx
colidx <- predict(glmnet.model,  type = "nonzero", s = gplmnet.model$lambda.min)
colidx
colnames(glmnet.x)[t(colidx)]
colidx <- predict(glmnet.model,  type = "coefficients", s = gplmnet.model$lambda.min)
colidx
names(colidx)
colidx[1][1]
order(abs(colidx), decreasing = T)
names(colidx)[order(abs(colidx), decreasing = T)]
rownames(colidx)
rownames(colidx)[order(abs(colidx), decreasing = T)]
gplmnet.model <- cv.glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = 10^seq(10,-2,length = 100))
glmnet.model <- glmnet(x = glmnet.x, y = glmnet.y[!is.na(glmnet.y)], family = 'binomial',lambda = gplmnet.model$lambda.min)
colidx <- predict(glmnet.model,  type = "coefficients", s = gplmnet.model$lambda.min)
as.data.frame(colidx)
colidx[1:5]
length(colidx)
features <- data.frame(features = rownames(colidx), coefs = colidx[1:length(colidx)]) %>% mutate(abscoef = abs(coefs))
ggplot(data = features, mapping = aes(y = abscoef, y = fct_reorder(features,abscoef))) + geom_point()
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip()
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw()
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.x = element_text(size = 10))
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 10))
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6))
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(0,5,col = 'red')
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(0,5)
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6))
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(xintercept = 37.5,linetype = "dashed", color = "red") + theme(legend.position = "none")
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(xintercept = 37.5,linetype = "dashed", color = "red") + theme(legend.position = "none") + xlab("Features") + ylab("Absolute Coefficients")
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(xintercept = 45.5,linetype = "dashed", color = "red") + theme(legend.position = "none") + xlab("Features") + ylab("Absolute Coefficients")
ggplot(data = features, mapping = aes(y = abscoef, x = fct_reorder(features,abscoef))) + geom_point() + coord_flip() + theme_bw() + theme(axis.text.y = element_text(size = 6)) + geom_vline(xintercept = 44.5,linetype = "dashed", color = "red") + theme(legend.position = "none") + xlab("Features") + ylab("Absolute Coefficients")
plot(gplmnet.model)
test.glmnet <- test[complete.cases(training),traincols]
glmnet.x.test <- model.matrix(TARGET_FLAG ~. , data = test.glmnet)[,-1]
glmnet.test <- predict(glmnet.model, newx = glmnet.x.test)
glmnet.test
glmnet.test <- predict(glmnet.model, newx = glmnet.x.test, response = 'prob')
head(glmnet.test)
roc(response = glmnet.y.test, predictor = glmnet.test)
glmnet.y.test= test.glmnet$TARGET_FLAG
roc(response = glmnet.y.test, predictor = glmnet.test)
roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)
test.glmnet <- test[complete.cases(training),traincols]
roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)
length(glmnet.y.test[!is.na(glmnet.y.test)])
dim(glmnet.test)
test.glmnet <- test[complete.cases(test),traincols]
glmnet.x.test <- model.matrix(TARGET_FLAG ~. , data = test.glmnet)[,-1]
glmnet.y.test= test.glmnet$TARGET_FLAG
glmnet.test <- predict(glmnet.model, newx = glmnet.x.test, response = 'prob')
roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)
plot(roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test),asf = NA)
plot(roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test),asp = NA)
plot(roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test),asp = NA,legacy.axes = T)
rocCurve.lasso <- roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)
rocCurve.lasso.train <- roc(response = glmnet.y.test[!is.na(glmnet.y.test)], predictor = glmnet.test)
plot(rocCurve.lasso.train,asp = NA,legacy.axes = T, col = 'red')
plot(rocCurve.lasso.train,asp = NA,legacy.axes = T, col = 'red')
text('top right',paste("AUC:", pROC::auc(rocCurve.lasso)))
text('topright',paste("AUC:", pROC::auc(rocCurve.lasso)))
text(0.2,0.7,paste("AUC:", pROC::auc(rocCurve.lasso)))
plot(rocCurve.lasso.train,asp = NA,legacy.axes = T, col = 'red')
text(0.2,0.7,paste("AUC:", pROC::auc(rocCurve.lasso)))
text(0.2,0.7,paste("AUC:", round(pROC::auc(rocCurve.lasso),2)),col = 'blue')
plot(rocCurve.lasso.train,asp = NA,legacy.axes = T, col = 'red')
text(0.2,0.7,paste("AUC:", round(pROC::auc(rocCurve.lasso),2)),col = 'blue')
glmnet.test
View(glmnet.test)
plot(glmnet.test)
plot(exp(glmnet.test))
glmnet.test.class <- predict(glmnet.model, newx = glmnet.x.test, response = 'response')
plot(glmnet.test.class, glmnet.test)
pROC::auc(rocCurve.fit1)
summary(training)
PotentialPred <- rownames(tr.bagged$importance)[order(tr.bagged$importance,decreasing = T)][1:12]
#df.rev.imputed <- VIM::kNN(df.rev.cleaned)
#
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
df.rev.cleaned$TARGET_FLAG2 <- ifelse(df.rev.cleaned$TARGET_FLAG==1,"Yes", "No")
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
training.models <- df.rev.cleaned[trainRows.models,]
test.models <- df.rev.cleaned[-trainRows.models,]
y <- ifelse(training.models$TARGET_FLAG2 == "No",0,1)
x <- as.matrix(training.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])])
dffit <- data.frame(y,x)
fit1 <- glm(y~.,family = binomial,data = dffit[complete.cases(dffit),])
summary(fit1)
rocCurve.fit1.train <- roc(response = dffit[complete.cases(dffit),"y"], predictor = fit1$fitted.values)
plot(rocCurve.fit1.train)
plot(rocCurve.fit1.train,legacy.axes = T, asp = NA)
auc(rocCurve.fit1.train)
pROC::auc(rocCurve.fit1.train)
summary(training)
PotentialPred <- rownames(tr.bagged$importance)[order(tr.bagged$importance,decreasing = T)][1:12]
#df.rev.imputed <- VIM::kNN(df.rev.cleaned)
#
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
training.models <- df.rev.cleaned[trainRows.models,]
test.models <- df.rev.cleaned[-trainRows.models,]
y <- ifelse(training.models$TARGET_FLAG2 == "No",0,1)
x <- as.matrix(training.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])])
dffit <- data.frame(y,x)
fit1 <- glm(y~.,family = binomial,data = dffit[complete.cases(dffit),])
summary(fit1)
rocCurve.fit1.train <- roc(response = dffit[complete.cases(dffit),"y"], predictor = fit1$fitted.values)
plot(rocCurve.fit1.train,legacy.axes = T, asp = NA)
pROC::auc(rocCurve.fit1.train)
x.test <- test.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])]
predictedClass <- ifelse(predict.glm(fit1,newdata = x.test, type = 'response') >= 0.5, 1,0)
test.y <- ifelse(test.models$TARGET_FLAG2 == "No",0,1)
table(predictedClass,test.y)
rocCurve.fit1 <- roc(response = test.y, predictor = predict.glm(fit1,newdata = x.test, type = 'response'))
plot(rocCurve.fit1, legacy.axes = T)
pROC::auc(rocCurve.fit1)
summary(training)
PotentialPred <- rownames(tr.bagged$importance)[order(tr.bagged$importance,decreasing = T)][1:12]
#df.rev.imputed <- VIM::kNN(df.rev.cleaned)
#
trainRows.models <- caret::createDataPartition(df.rev.cleaned$TARGET_FLAG2,p = 0.75, list = F)
training.models <- df.rev.cleaned[trainRows.models,]
test.models <- df.rev.cleaned[-trainRows.models,]
y <- ifelse(training.models$TARGET_FLAG2 == "No",0,1)
x <- as.matrix(training.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])])
dffit <- data.frame(y,x)
fit1 <- glm(y~.,family = binomial,data = dffit[complete.cases(dffit),])
summary(fit1)
rocCurve.fit1.train <- roc(response = dffit[complete.cases(dffit),"y"], predictor = fit1$fitted.values)
plot(rocCurve.fit1.train,legacy.axes = T, asp = NA)
pROC::auc(rocCurve.fit1.train)
x.test <- test.models[ ,c(PotentialPred[-7],colnames(training.models) [grep("JOB_", colnames(training.models))])]
predictedClass <- ifelse(predict.glm(fit1,newdata = x.test, type = 'response') >= 0.5, 1,0)
test.y <- ifelse(test.models$TARGET_FLAG2 == "No",0,1)
table(predictedClass,test.y)
rocCurve.fit1 <- roc(response = test.y, predictor = predict.glm(fit1,newdata = x.test, type = 'response'))
plot(rocCurve.fit1, legacy.axes = T)
pROC::auc(rocCurve.fit1)
