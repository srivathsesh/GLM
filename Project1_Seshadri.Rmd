---
title: "Project1"
author: "Sri Seshadri"
date: "9/30/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
library(magrittr)
library(dplyr)
```

## 1. Introduction



## 2. Exploratory Data Analysis (EDA)


### 2.1 About the data


```{r}
# Read the data in
MB <- read.csv('MoneyBall.csv')
cols <- colnames(MB)
library(mosaic)
# Compute overall statistic and show missing values
sanitycheck <- do.call(rbind,dfapply(MB,favstats, select = is.numeric))
rowname <- rownames(sanitycheck)
sanitycheck <- sanitycheck %>%  mutate(cv = sd/mean)
rownames(sanitycheck) <- rowname
knitr::kable(round(sanitycheck,2), caption = "Summary Stats and missing values")
```

### 2.2 Missing data

Where are they from? Systemic pattern?

```{r, fig.width=8}
# indicator variables for record missing
MB <- MB %>% 
        mutate(Missing_BatSO = as.logical(ifelse(is.na(TEAM_BATTING_SO),1,0))) %>%
        mutate(Missing_BasSB = as.logical(ifelse(is.na(TEAM_BASERUN_SB),1,0))) %>% 
        mutate(Missing_BasCS = as.logical(ifelse(is.na(TEAM_BASERUN_CS),1,0))) %>% 
        mutate(Missing_PitSO = as.logical(ifelse(is.na(TEAM_PITCHING_SO),1,0))) %>% 
        mutate(Missing_FieldDP = as.logical(ifelse(is.na(TEAM_FIELDING_DP),1,0)))  
MB$Filter <- as.logical(ifelse(rowSums(MB[,18:22]) > 0 ,1, 0))
library(ggplot2)
# p0 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = Missing_BatSO, fill = Missing_BatSO)) + geom_histogram(alpha = 0.5)

p1 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = Filter, fill = Filter)) + geom_histogram(alpha = 0.5) + theme_bw()

p2 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_SO, color = Filter, fill = Filter)) + geom_histogram(alpha = 0.5) + theme_bw()
gridExtra::grid.arrange(p1,p2,ncol = 2)

```

### 2.3 Relationships among variables


```{r}
cordata <- MB %>% dplyr::select(cols[c(-1,-11)]) 
cordata <- cordata[complete.cases(cordata),]
corrplot::corrplot(cor(cordata),tl.cex = 0.8)
```
#### 2.3.1 Exploring the relationships further

What are the linear relationships with varying slopes - parallel lines?

```{r}
ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_H, y = TEAM_BATTING_H, color = Filter)) + geom_point()
ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_H, fill = Filter, color = Filter)) + geom_histogram(alpha = 0.5) + facet_grid(Filter ~ .)

MB <- MB %>% 
  dplyr::mutate(Pitch_H_Outlier = as.logical(ifelse(TEAM_PITCHING_H >= 1890, 1,0))) 
MB_test <- MB %>% filter(Filter == F)
  ggplot(data = MB_test,mapping = aes(x = TEAM_PITCHING_H, color = Pitch_H_Outlier, fill = Pitch_H_Outlier)) + geom_histogram(alpha = 0.5) + theme_bw()
  
ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_H, y = TEAM_BATTING_H, color = Pitch_H_Outlier )) + geom_point() + theme_bw()

ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_SO, y = TEAM_BATTING_SO, color = Pitch_H_Outlier )) + geom_point() + theme_bw()

ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_BB, y = TEAM_BATTING_BB, color = Pitch_H_Outlier )) + geom_point() + theme_bw()

ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = Pitch_H_Outlier )) + geom_point() + theme_bw()



```


#### 2.3.2  Indicator variables 

```{r}
MB <- MB  %>% 
    mutate(BatHR_Filter = as.logical(ifelse(TEAM_BATTING_HR <= 59, 1,0))) %>% 
    mutate(BatSO_Filter = as.logical(ifelse(TEAM_BATTING_SO <= 250, 1,0))) %>% 
    mutate(PitSO_Filter = as.logical(ifelse(TEAM_PITCHING_SO > 2000, 1, 0)))
colnames(MB)[1] <- "INDEX"
Pitch_H_outlierT <- MB$INDEX[MB$Pitch_H_Outlier]
BatSO_Filter_T <- MB$INDEX[MB$BatSO_Filter]
PitSOFilter_T <- MB$INDEX[MB$PitSO_Filter]

MB <- MB %>% mutate (flag = as.logical(ifelse(INDEX %in% setdiff(setdiff(Pitch_H_outlierT,BatSO_Filter_T), PitSOFilter_T), 1,0)))

MB$HRSO_Filter <- as.logical(ifelse(rowSums(MB[,c('BatHR_Filter','BatSO_Filter')]) > 0 ,1, 0))


ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = Pitch_H_Outlier )) + geom_point() + theme_bw()
ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = flag)) + geom_point() + theme_bw()
ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = BatSO_Filter)) + geom_point() + theme_bw()
ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = PitSO_Filter)) + geom_point() + theme_bw()

ggplot(data = cordata, mapping = aes(y = TARGET_WINS, x = TEAM_BATTING_HR, color = flag)) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
ggplot(data = cordata, mapping = aes(y = TARGET_WINS, x = TEAM_BATTING_HR, color = as.factor(Pitch_H_Outlier))) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
ggplot(data = cordata, mapping = aes(x = TEAM_FIELDING_E, y = TARGET_WINS, color = as.factor(Pitch_H_Outlier))) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
ggplot(data = cordata, mapping = aes(x = TEAM_FIELDING_E, y = TARGET_WINS, color = flag)) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
```

## 3.0 Feature selections

What are the important features amongst complete cases?

```{r}
reqcols <- colnames(MB)[c(-1,-11,-18:-23,-25,-29)]
cordata <- MB %>% dplyr::select(reqcols) 
reqrows <- complete.cases(cordata)
cordata <- cordata[reqrows,]
# remove the lower end of the Batting SO and outliers in Pitching SO

cordata <- cordata %>% 
            dplyr::filter(BatSO_Filter == !T) %>% 
            dplyr::filter(PitSO_Filter == !T)
```

Would a simple model work?

```{r}
colnames(cordata)
simplemod <- lm(formula = TARGET_WINS ~ . , data = cordata)
summary(simplemod)
colnames(cordata)
```


### 3.1 PCA

```{r}

x <- model.matrix(TARGET_WINS ~., data = cordata)[,-1]
colnames(x)[15:18] <- c("Pitch_H_Outlier","BatSO_Filter", "PitSO_Filter","flag")
pca <- princomp(x[,-15:-18],cor = T)
summary(pca)
cumvariance <- cumsum(pca$sdev^2) / sum(pca$sdev^2)
plot(cumvariance, xlab = "Component Number", ylab = "Cumulative Variance", type = "l")
points(cumvariance)

pca.scores <- pca$scores[,1:6]
#df <- data.frame(pca.scores,MB[reqrows,colnames(MB)[c(2,24)]])
df <- data.frame(pca.scores, x[,c(15,18)],MB[reqrows,2])
colnames(df)[9] <- c("TARGET_WINS")
library(MASS)
lower.lm <- lm(data = df, formula = TARGET_WINS ~ Comp.1 + Comp.2 + Comp.3 + Comp.4 + Comp.5 + Comp.6 + flag)
upper.lm <- lm(data = df, formula = TARGET_WINS ~ .)
step.lm <- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "both", trace = F)
summary(step.lm)
anova(step.lm)
plot(step.lm)
car::vif(step.lm)
```

### 3.2 Lasso

```{r}
library(glmnet)
grid = 10^seq(10,-2,length = 100)
lasso.out <- cv.glmnet(x,df$TARGET_WINS,alpha = 1)
plot(lasso.out)
lasso.mod <- glmnet(x,df$TARGET_WINS,alpha = 1, lambda = lasso.out$lambda.min)

lasso.lm$anova

# R Squared
lasso.mod$dev.ratio

Candidate_Lasso <- names(coef(lasso.mod)[order(abs(round(coef(lasso.mod),3)),decreasing = T),])[2:10]
formula <- paste("TARGET_WINS ~ ",paste(Candidate_Lasso, collapse = " + "))

lasso.selection.lm <- stepAIC(lm(TARGET_WINS ~ 1, data = cordata), scope = list(lower = ~1, upper = formula), direction = "both", trace = F) 
summary(lasso.selection.lm)
car::vif(lasso.selection.lm)


# cordata$Pitch_H_Outlier <- as.numeric(cordata$Pitch_H_Outlier)
# summary(lm(data = cordata ,  TARGET_WINS ~ flag + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_BATTING_H  + TEAM_BASERUN_CS ))
```

### 3.3 More feature selection - VIF regression

```{r}
vif.lm <- VIF::vif(df$TARGET_WINS,x)
vifdata <- data.frame(vif.lm$modelmatrix,scale(df$TARGET_WINS))
vif.lm$select

summary(lm(data = vifdata, formula = scale.df.TARGET_WINS. ~ .))
testmoel <- lm(formula = )
```

### 3.4 Partial Least Squares (PLS) Regression

How to add indicator variables to the model?

```{r}
plsdf <- data.frame(x[,-15:-18], cordata$TARGET_WINS)
names(plsdf)[15] <- "TARGET_WINS"
pls.lm <- pls::plsr(TARGET_WINS ~ ., data  = plsdf, scale = T, validation = "CV")

pls::validationplot(pls.lm, val.type = "RMSEP")
summary(pls.lm)
plot(varImp(pls.lm),top = 10)
plot(pls::R2(pls.lm))
```


### 3.5 Random forest

```{r}
# Split data into training and testing
cordata <- cordata %>% mutate(ID = 1:nrow(cordata))
train <- dplyr::sample_frac(cordata,0.7)
test <- dplyr::anti_join(cordata, train)


```


### Impute data

```{r}
library(VIM)
MB2 <- kNN(data = MB,variable = colnames(MB)[c(-1,-11,-18:-27)], k = 5 )
sanitycheck2 <- do.call(rbind,dfapply(MB2,favstats, select = is.numeric))
MB2 <- MB2 %>% subset(select = rownames(sanitycheck2))
MB2 <- MB2[,c(-1,-11)]
x1 <- model.matrix(TARGET_WINS ~ ., data = MB2)
pca2 <- princomp(x1,cor = T)
cumvariance2 <- cumsum(pca2$sdev^2) / sum(pca2$sdev^2)
plot(cumvariance2, xlab = "Component Number", ylab = "Cumulative Variance", type = "l")
points(cumvariance2)
pca2.scores <- pca2$scores[,1:6]
df2 <- data.frame(pca2.scores,MB2$TARGET_WINS)
colnames(df2)[7] <- 'TARGET_WINS'
lower2.lm <- lm(data = df2, formula = TARGET_WINS ~ 1)
upper2.lm <- lm(data = df2, formula = TARGET_WINS ~ .)
step2.lm <- stepAIC(lower2.lm, scope = list(upper = formula(upper2.lm), lower = ~1), direction = "both", trace = F)
summary(step2.lm)
anova(step2.lm)
plot(step2.lm)
```



