---
title: "Project1"
author: "Sri Seshadri"
date: "9/30/2017"
output: 
  pdf_document:
    toc : true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F,tidy.opts=list(width.cutoff=70),tidy=TRUE)
library(magrittr)
library(dplyr)
```

\pagebreak


# 1. Introduction

An sports investor is interested in investing in the sport of baseball. The investor is looking at investments ranging from team ownership, collectibles, apparel and legal sports betting. There is interest in understanding what are factors that influence teams' winning and predicting the number of wins in a season, so that an informed decision is made on the investment. To enable decision making, historical performance of baseball teams from the year 1871 to 2006 is analyzed and modeled for predicting number of wins in a season. This report discusses the analysis and the predictive models developed using the historical data. It should be noted that the data scientist who analyzed the data does not have subject matter expertise in the area of baseball. It is expected that the model(s) may need fine tuning based on feedback from subject matter experts. 

## 1.1 Analysis Process

The following process steps were used for building a predicitve models:

* Exploratory Data Analysis
    + Perform data quality checks, quantify missing data.
      - Check for systemic loss in data
    + Understand relationships amongst predictors and between target variables and predictors.
      - Create attribure or indicator variables to aid data cleaning.
      - Filter out clean data for feature selection and model building.
* Feature Selection
    + Subset complete records to model wins in season
      - Use different modeling techniques to select candidate predictors.
      - If data is missing for candidate predictors, identify imputing methods.
* Model Building
    + Test models that were build using complete records on the entire data set with imputed data.
    + Compare models based on Adjusted R squared, AIC, MAE
      - Check if models make physical sense. 
* Initial model deployment
    + Deploy model to predict wins on out of sample data.
    + Discuss models and results with subject matter experts.
    + Fine tune model and re-test
* Final model deployment
    + Investment decisions.
  

## 1.2 Executive summary

The below model was chosen from a candidate pool of 10 models (7 models are discussed in this report) for its simplicity, interpretability and comparable mean absolute error.

**TARGET_WINS = 34.12 + 0.01 * TEAM_BATTING_H + 0.1 * TEAM_BATTING_3B + 0.12 * TEAM_BATTING_HR + 0.04 * TEAM_BATTING_BB - 0.02 * TEAM_BATTING_SO + 0.07 * TEAM_BASERUN_SB - 5.49 * BatHR_Filter**

The model has an average error (MAE) of Â± 11 wins. The data scientist welcomes suggestions from subject matter expertise to further iterate on the model building process to produce a better model. 


# 2. About the Data

The data provided has approximately 2300 rows, each representing a team's performance in the seasons during the years 1871 to 2006. The statistics are adjusted to match a performance of 162 game season. The dictionary of the variables in the data are provided in appendix A.1. Table 1 below shows statistics of the variables in the data. 

It can be seen that there are multiple variables with missing data. The systemic nature of missing data is discussed in the next section, the pattern in missing leads us to assume that the missing data is likely from the 18th and the early 19th century, where sophisticated data collection may not have existed. If the variables that contain missing data are deemed important for prediciting number of wins, then an appropriate method would be used for data imputation. 


```{r}
# Read the data in
MB <- read.csv('MoneyBall.csv')
cols <- colnames(MB)
library(mosaic)
# Compute overall statistic and show missing values
sanitycheck <- do.call(rbind,dfapply(MB,favstats, select = is.numeric))
rowname <- rownames(sanitycheck)
sanitycheck <- sanitycheck %>%  mutate(cv = sd/mean)
rownames(sanitycheck) <- rowname
knitr::kable(round(sanitycheck,2), caption = "Summary Stats and missing values")
```

# 3. Exploratory Data Analysis (EDA) & Data Preparation

## 3.1 Missing data

Figure 1, shows that the missing data is systematic and they correspond to the lower end of the distributions of home runs by batters and strike-outs. Its also seen that the home-runs and strike-outs are bi-modal in nature. The complete cases from the data are used to explore the relationship amongst the predictor variables and the relationship between Wins and the other predictors. The exploratory data analysis is used as a tool for feature selection for model building.

```{r, fig.width=8,fig.cap='Rows with missing data is from the lower end of the distributions of Home runs and Strike outs'}
# indicator variables for record missing
MB <- MB %>% 
        mutate(Missing_BatSO = as.logical(ifelse(is.na(TEAM_BATTING_SO),1,0))) %>%
        mutate(Missing_BasSB = as.logical(ifelse(is.na(TEAM_BASERUN_SB),1,0))) %>% 
        mutate(Missing_BasCS = as.logical(ifelse(is.na(TEAM_BASERUN_CS),1,0))) %>% 
        mutate(Missing_PitSO = as.logical(ifelse(is.na(TEAM_PITCHING_SO),1,0))) %>% 
        mutate(Missing_FieldDP = as.logical(ifelse(is.na(TEAM_FIELDING_DP),1,0)))  
MB$Missing <- as.logical(ifelse(rowSums(MB[,18:22]) > 0 ,1, 0))
library(ggplot2)
# p0 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = Missing_BatSO, fill = Missing_BatSO)) + geom_histogram(alpha = 0.5)

p1 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = Missing, fill = Missing)) + geom_histogram(alpha = 0.5) + theme_bw()

p2 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_SO, color = Missing, fill = Missing)) + geom_histogram(alpha = 0.5) + theme_bw()
gridExtra::grid.arrange(p1,p2,ncol = 2)

```

## 3.2 Correlations in the data

Figure 2 shows the correlation plot of the variables in the data. It is seen that team batting home runs and pitching home runs are strongly correlated amd so are walks by batters and walks allowed when pitching. Similary for the strike outs. The fielding errors and home runs are negatively correlated; so is triples with fielding errors. It will be interesting to understand the reason for this behaviour from the subject matter experts. 

The target wins variable have weak correlations with many variables without a strong correlation with any of the variables in the data. The combined contributions of the predictor variables to explain the number of wins is explored in the sections below.

```{r, fig.cap= 'Correlation plot',fig.width=4}
cordata <- MB %>% dplyr::select(cols[c(-1,-11)]) 
cordata <- cordata[complete.cases(cordata),]
corrplot::corrplot(cor(cordata),tl.cex = 0.8)
```



## 3.3 Relationship between variables.

Left panel of Figure 3 shows the extreme values in hits allowed by teams and they correspond to rows where some variables have missing values. Zooming in on the set of data with complete cases (no missing data in any of the columns), it is seen that hits allowed has two contiguous distributions. The right panel of figure 3 shows the extreme values. Figure 4 shows the change in relationship amongst Base hits and hits allowed; strikeouts by pitcher; walks allowed by pitchers and walks by batters; is associated with the extreme values in hits allowed by pitchers.

```{r, fig.cap= "Extreme values in Hits allowed ", fig.height= 5}
#ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_H, fill = Missing, color = Missing)) + geom_histogram(alpha = 0.5) + facet_grid(Missing ~ .)
ED1 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_H, y = TEAM_BATTING_H, color = Missing)) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )


MB <- MB %>% 
  dplyr::mutate(Pitch_H_Outlier = as.logical(ifelse(TEAM_PITCHING_H >= 1890, 1,0))) 
MB_test <- MB %>% filter(Missing == F)
ED2 <- ggplot(data = MB_test,mapping = aes(x = TEAM_PITCHING_H, color = Pitch_H_Outlier, fill = Pitch_H_Outlier)) + geom_histogram(alpha = 0.5) + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
  
gridExtra::grid.arrange(ED1,ED2, ncol = 2)


```

```{r, fig.cap= "Distinct relationships between variables attributed to outliers in Hits allowed"}
ED3 <- ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_H, y = TEAM_BATTING_H, color = Pitch_H_Outlier )) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )

ED4 <- ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_SO, y = TEAM_BATTING_SO, color = Pitch_H_Outlier )) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )

ED5 <- ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_BB, y = TEAM_BATTING_BB, color = Pitch_H_Outlier )) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )

ED6 <- ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = Pitch_H_Outlier )) + geom_point() + theme_bw()+ theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )

gridExtra::grid.arrange(ED3,ED4,ED5,ED6, nrow = 2, ncol = 2)
```

\pagebreak

### 3.3.1  Indicator variables 

To effectively capture the various phenomena (such as the difference in slopes in Fig 4) either attributed to or associated with various sections in the data, indicator variables are used. Indicator variables can be used as dummy variables in regression models to model the change in behaviors. The following table lists the indicator variables created for effective modeling. Figure 5 and 6 shows the indicator variables capturing the intended section of the population.

```{r}
indicators <- data.frame(Name = c("Pitch_H_Outlier","BatHR_Filter","BatSO_Filter","PitSO_Filter","AltSlope"), Purpose = c("Outlier filter", "bimodal capture", "Outlier filter","Outlier filter","Capture parallel slope" ), Criteria = c("TEAM_PITCHING_H >= 1890","TEAM_BATTING_HR <= 59","TEAM_BATTING_SO <= 250","TEAM_PITCHING_SO","setdiff(setdiff(Pitch_H_outlierT,BatSO_Filter_T), PitSOFilter_T)"))
library(kableExtra)
knitr::kable(indicators,caption = "Indicator variables description") %>% 
  kable_styling(font_size = 7)
```


```{r, fig.cap = "indicator variables",fig.height= 4}
MB <- MB  %>% 
    mutate(BatHR_Filter = as.logical(ifelse(TEAM_BATTING_HR <= 59, 1,0))) %>% 
    mutate(BatSO_Filter = as.logical(ifelse(TEAM_BATTING_SO <= 250, 1,0))) %>% 
    mutate(PitSO_Filter = as.logical(ifelse(TEAM_PITCHING_SO > 2000, 1, 0)))

I1 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = BatHR_Filter, fill = BatHR_Filter)) + geom_histogram(alpha = 0.5) + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
I2 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_SO, color = BatSO_Filter, fill = BatSO_Filter)) + geom_histogram(alpha = 0.5) + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
gridExtra::grid.arrange(I1,I2,ncol = 2, nrow = 2)

```


```{r, fig.cap="Indicator variables contd"}
 #MB  %>%  dplyr::filter(Missing == T) %>% 
#ggplot(mapping = aes(x = TEAM_PITCHING_SO, fill = PitSO_Filter, color = PitSO_Filter )) + geom_histogram(alpha = 0.5) + theme_bw()
colnames(MB)[1] <- "INDEX"
Pitch_H_outlierT <- MB$INDEX[MB$Pitch_H_Outlier]
BatSO_Filter_T <- MB$INDEX[MB$BatSO_Filter]
PitSOFilter_T <- MB$INDEX[MB$PitSO_Filter]

MB <- MB %>% mutate (AltSlope = as.logical(ifelse(INDEX %in% setdiff(setdiff(Pitch_H_outlierT,BatSO_Filter_T), PitSOFilter_T), 1,0)))

MB$HRSO_Filter <- as.logical(ifelse(rowSums(MB[,c('BatHR_Filter','BatSO_Filter')]) > 0 ,1, 0))


I3 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = Pitch_H_Outlier )) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
I4 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = AltSlope )) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
I5 <- ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = AltSlope)) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
I6 <- ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = BatSO_Filter)) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
I7 <- ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = PitSO_Filter)) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
I8 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color =  BatSO_Filter)) + geom_point() + theme_bw() + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
gridExtra::grid.arrange(I3,I4,I5,I6,I7,I8, ncol = 2, nrow = 3)
```

### 3.3.2 Target Wins Vs Potential predictors

The effect of indicator variables on relationship between Target Wins and potential predictors is showm in figure 7. The data is subsetted to exclude outliers in strike-outs by batters and pitchers.

```{r,fig.cap="Target Wins Vs Predictors by AltSlope and Pitch_H_Outlier"}
reqcols <- colnames(MB)[c(-1,-11,-18:-23,-29)]
cordata <- MB %>% dplyr::select(reqcols) 
reqrows <- complete.cases(cordata)
cordata <- cordata[reqrows,]
# remove the lower end of the Batting SO and outliers in Pitching SO

cordata <- cordata %>% 
            dplyr::filter(BatSO_Filter == !T) %>% 
            dplyr::filter(PitSO_Filter == !T) 
cordata$Pitch_H_Outlier <- as.factor(cordata$Pitch_H_Outlier)

T1 <- ggplot(data = cordata, mapping = aes(y = TARGET_WINS, x = TEAM_BATTING_HR, color = AltSlope)) + geom_point() + theme_bw()+ geom_smooth(method = "lm") + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
T2 <- ggplot(data = cordata, mapping = aes(y = TARGET_WINS, x = TEAM_BATTING_HR, color = Pitch_H_Outlier)) + geom_point() + theme_bw()+ geom_smooth(method = "lm") + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
T3 <- ggplot(data = cordata, mapping = aes(x = TEAM_FIELDING_E, y = TARGET_WINS, color = Pitch_H_Outlier)) + geom_point() + theme_bw()+ geom_smooth(method = "lm") + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )
T4 <- ggplot(data = cordata, mapping = aes(x = TEAM_FIELDING_E, y = TARGET_WINS, color = AltSlope)) + geom_point() + theme_bw()+ geom_smooth(method = "lm") + theme(axis.title = element_text(size = 8),legend.title = element_text(size = 8) )

gridExtra::grid.arrange(T1,T2,T3,T4,ncol=2,nrow = 2)
cordata$Pitch_H_Outlier <- as.numeric(cordata$Pitch_H_Outlier)
```

\pagebreak

# 4. Feature selections & Model building

The feature selections and Model building steps of the analysis process are iterative. The modeling process begins with a selection of subset of variables that are theoretically correlated with wins. When predictors are related or correalted to eachother, refer figure 2, only one of the predictors from the pair of correlated variables is selected for modeling. The feature selection and model building process is initially built on complete cases or records. Then model is then tested on the entire data set after filling in the missing values (if needed) by imputation.

In the sections below, several models are discussed such as:

1. Simple multiple regression models, with manual feature selection (2 nos).
2. Stepwise automated variable selection.
3. Principle components regression (PCR).
4. Lasso regression variable selection and OLS regression.
5. Partial Least Squares regression (PLS).
6. Random forest feature selection and OLS regression.

```{r}

simple.Pred <- colnames(cordata)[c(1:9,15:17)]
simple.Fit <- lm(formula = TARGET_WINS ~ . , data = cordata[,simple.Pred])

## Generating equation
coef.simple <- round(coefficients(simple.Fit),2)
signs.simple <- ifelse(sign(coef.simple)==1,"+", "-")
Betas.simple <- paste(abs(coef.simple[2:length(coef.simple)]),"*",simple.Pred[-1])
eqn.simple <- paste("TARGET_WINS =", paste(coef.simple[1],paste(paste(signs.simple[2:length(signs.simple)],Betas.simple),collapse = " ")))



simple.rev.Pred <- simple.Pred[which(simple.Pred %in% c("TEAM_BASERUN_CS","Pitch_H_Outlier","TEAM_BATTING_2B","TEAM_FIELDING_DP")) * -1]
simple.rev.Fit <- lm(formula = TARGET_WINS ~ . , data = cordata[,simple.rev.Pred])

## Generating equation
coef.simple.rev <- round(coefficients(simple.rev.Fit),2)
signs.simple.rev <- ifelse(sign(coef.simple.rev)==1,"+", "-")
Betas.simple.rev <- paste(abs(coef.simple.rev[2:length(coef.simple.rev)]),"*",simple.rev.Pred[-1])
eqn.simple.rev <- paste("TARGET_WINS =", paste(coef.simple.rev[1],paste(paste(signs.simple.rev[2:length(signs.simple.rev)],Betas.simple.rev),collapse = " ")))

```

## 4.1 Multiple linear regression (Simple models)

The following model was fit with manual choice of variables by refering the correlation plot in figure 2. When manual choice of features/ variables were made, the theoretical effect was born in mind. However, no subject matter expertise was used in selection.

**`r eqn.simple`**

It is found that "BASERUN_CS" and "PITCH_H_Outlier" variables were statistically not significant from zero. Also the model meshes with theoretical effect for most part except that the coefficent for FIELDING_DP is negative, counter to expectation. The adjusted R-squared of 35% leaves room for improvement. The assumption of OLS regression have been satisfied.

```{r, fig.cap="Simple model results"}
# Simple model

summary(simple.Fit)
#car::vif(simple.Fit)
par(mfrow = c(2,2))
plot(simple.Fit,which = 1:3)
plot(simple.Fit,which = 4)
# length(simple.Pred)

```

## 4.2 Revisied simple multiple linear regression model

From the above model, the variables "BASERUN_CS" and "PITCH_H_Outlier" are removed and a revised model is fit as below:

**`r eqn.simple.rev `**

This model is interpretable and is in line with theoretical expectations. The indicator variable correctly identifies the effect of low values of home runs leading up to fewer predicted wins.  However adjusted R-Squared value of 32% leaves room for improvement. The assumptions of OLS regression is satisfied and multicollinearity is satisfactory.

```{r,fig.cap="Revised Simple model results"}
summary(simple.rev.Fit)
#car::vif(simple.rev.Fit)
par(mfrow = c(2,2))
plot(simple.rev.Fit, which = 1:3)
plot(simple.rev.Fit,which = 4)

```

## 4.3 Automated Variable selection

```{r}
Simple.step.Fit <- MASS::stepAIC(simple.rev.Fit, scope = list(lower = ~1, upper = formula(simple.rev.Fit)),direction = "both",trace = F)
formula_simple.step <- as.character(formula(Simple.step.Fit)) [3]
Simple.step.pred <- unlist(strsplit(formula_simple.step, split = "+",fixed = T))
coef.simple.step <- round(coefficients(Simple.step.Fit),2)
signs.simple.step <- ifelse(sign(coef.simple.step)==1,"+", "-")
Betas.simple.step <- paste(abs(coef.simple.step[2:length(coef.simple.step)]),"*",Simple.step.pred)
eqn.simple.step <- paste("TARGET_WINS =", paste(coef.simple.step[1],paste(paste(signs.simple.step[2:length(signs.simple.step)],Betas.simple.step),collapse = " ")))
```

Automated stepwise variable selection method was employed with a search range between full model (all variables) and intercept only variables. The resulting model had high multicollinearity and the model is not shown in this report. However, automated variable selection was used to search within the manually chosen variables used in the above two models. The following model was fit as a result:

**`r eqn.simple.step`**

The resulting model was identical to the revised simple model.

```{r, fig.cap= "Stepwise automated variable selection"}
summary(Simple.step.Fit)
par(mfrow = c(2,2))
plot(Simple.step.Fit,which = 1:4)


```


### 4.4 Principle Component Regression (PCA)

Given that the predictors are correlated and full models having the problem of multicollinearity, Principle Component Analysis (PCA) was employed for overcoming the problem of multicollinearity in search of a predictive model by giving up interpretability. It is seen that the first 6 principle components explain over 85 % of the variation in the data. Figure 12 shows the clusters in the data, that's explained by the indicator variables. It is seen that there are 3 clsuters. Cluster 1 indicating Pitch_H_outlier being true; Cluster 1.1 indicating both Pitch_H_outlier and BatHR_Filter (low home runs) being true; Clsuter 2 indicating alternate slope. Refer section 3.3.1 for indicator variable definitions.

```{r,fig.cap= "Proportion of variance explained by the pricinple components"}

# cleanup data for PCA

nearzeros <- caret::nearZeroVar(cordata,saveMetrics = T)
zerovars <- rownames(nearzeros)[nearzeros$zeroVar]
pcavars <- colnames(cordata)[c(1,which(colnames(cordata) %in% zerovars))*-1]
pcadata <- cordata[,pcavars]

pca <- princomp(pcadata,cor = T)

#summary(pca)
# x <- model.matrix(TARGET_WINS ~., data = cordata)[,-1]
# colnames(x)[15:19] <- c("Pitch_H_Outlier","Bat_HR_Filter","BatSO_Filter", "PitSO_Filter","flag")
# pca <- princomp(x[,-17:-18],cor = T)
# summary(pca)
cumvariance <- cumsum(pca$sdev^2) / sum(pca$sdev^2)
plot(cumvariance, xlab = "Component Number", ylab = "Cumulative Variance", type = "l")
points(cumvariance)
```

```{r, fig.cap = "clusters"}
pca.scores <- pca$scores[,1:6]
#df <- data.frame(pca.scores,MB[reqrows,colnames(MB)[c(2,24)]])
dfclust <- data.frame(pca.scores,cordata[,c('Pitch_H_Outlier','BatHR_Filter','AltSlope', 'TARGET_WINS')])
dfclust <- dfclust %>% dplyr::mutate(cluster = as.factor(Pitch_H_Outlier + BatHR_Filter/10))
#colnames(df)[7] <- c("TARGET_WINS")
clust1 <- ggplot(data = dfclust, mapping = aes(x = Comp.1, y = Comp.2, color = cluster )) + geom_point() + theme_bw()
clust2 <- ggplot(data = dfclust, mapping = aes(x = Comp.1, y = Comp.2, color = AltSlope)) + geom_point() + theme_bw()
library("RColorBrewer")
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
sc <- scale_colour_gradientn(colours = myPalette(100))
clust3 <- ggplot(data = dfclust, mapping = aes(x = Comp.1, y = Comp.2, color = TARGET_WINS)) + geom_point() + theme_bw() + scale_colour_gradient2() + sc
gridExtra::grid.arrange(clust1,clust2,clust3, ncol=2)
```



```{r}
library(MASS)
lower.lm <- lm(data = dfclust[,c(1:6,10)], formula = TARGET_WINS ~ 1)
upper.lm <- lm(data = dfclust[,c(1:6,10)], formula = TARGET_WINS ~ .)
pca.step.Fit <- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "both", trace = F)

formula_pca <- as.character(formula(pca.step.Fit))[3]
predictors_pca <- unlist(strsplit(formula_pca, split = "+",fixed = T))
coef.pca.step <- round(coefficients(pca.step.Fit),2)
signs.pca.step <- ifelse(sign(coef.pca.step)==1,"+", "-")
Betas.pca.step <- paste(abs(coef.pca.step[2:length(coef.pca.step)]),"*",predictors_pca)
eqn.pca.step <- paste("TARGET_WINS =", paste(coef.pca.step[1],paste(paste(signs.pca.step[2:length(signs.pca.step)],Betas.pca.step),collapse = " ")))
```

The following PC model was fit :

**`r eqn.pca.step`**

The PCA regression did not yield an interpretable model and also the model's R-Squared value of 235 was less than ideal.

```{r, fig.cap = "PCA Regression"}
summary(pca.step.Fit)
anova(pca.step.Fit)
par(mfrow=c(2,2))
plot(pca.step.Fit, which = 1:4)


pca.forward.Fit <- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "forward", trace = F)
#summary(pca.forward.Fit)

#plot(pca.forward.Fit)

pca.backward.Fit<- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "backward", trace = F)
# summary(pca.backward.Fit)
# plot(pca.backward.Fit)

# The below code just replicates the pca.step.Fit for easy prediction for testing. Its a change in class object.

pcadatarev <- data.frame(TARGET_WINS = cordata[,1],pcadata)
pca.Fit <- pls::pcr(data = pcadatarev, formula = TARGET_WINS ~ .,scale = T,ncomp = 6)



```

### 4.5 Lasso Regression.

The variable selection property of Lasso regression is leveraged and an OLS fit is made on the variables selected by Lasso regression. Figure 12 shows the variable importance plot. The variables chosen are similar to the ines chosen manually. 

```{r}
library(glmnet)
x <- model.matrix(TARGET_WINS ~., data = cordata)[,-1]
colnames(x) <- gsub(pattern = "TRUE", "", colnames(x))
y <- cordata$TARGET_WINS
grid = 10^seq(10,-2,length = 100)
lasso.out <- cv.glmnet(x,y,alpha = 1)
#plot(lasso.out)
lasso.Fit <- glmnet(x,y,alpha = 1, lambda = lasso.out$lambda.min)
#summary(lasso.Fit)
Candidate_Lasso <- names(coef(lasso.Fit)[order(abs(round(coef(lasso.Fit),3)),decreasing = T),])[2:10]
Coeff.Lasso <- coef(lasso.Fit)[order(abs(round(coef(lasso.Fit),3)),decreasing = T)][2:10]
Candidate_Lasso <- gsub(pattern = "TRUE",replacement = "",x = Candidate_Lasso)
lasso.formula <- paste("TARGET_WINS ~ ",paste(Candidate_Lasso, collapse = " + "))

ImpPredictors.Lasso <- data.frame(Predictors = Candidate_Lasso,Importance = abs(Coeff.Lasso))

library(forcats)
ggplot(data = ImpPredictors.Lasso,mapping = aes(y = Importance, x = fct_reorder(Predictors,Importance))) + geom_col(color = 'red', fill = 'red') + coord_flip() + theme_bw() + xlab("Predictors")
# Proper naming of variables

```



```{r}
# R Squared
#lasso.Fit$dev.ratio

lasso.pred <- predict(lasso.Fit,s = lasso.out$lambda.min,newx =x)
lasso.resid <- y - as.numeric(lasso.pred)
# plot(y = lasso.resid, x = lasso.pred)
# qqnorm(lasso.resid)
# qqline(lasso.resid)

Lasso.OLS.Fit <- lm(data = cordata[,c('TARGET_WINS',Candidate_Lasso)], formula = TARGET_WINS ~ .)
formula_Lasso.OLS <- as.character(formula(Lasso.OLS.Fit))[3]
predictors_Lasso.OLS <- unlist(strsplit(formula_Lasso.OLS, split = "+",fixed = T))
coef.Lasso.OLS <- round(coefficients(Lasso.OLS.Fit),2)
signs.Lasso.OLS <- ifelse(sign(coef.Lasso.OLS)==1,"+", "-")
Betas.Lasso.OLS <- paste(abs(coef.Lasso.OLS[2:length(coef.Lasso.OLS)]),"*",predictors_Lasso.OLS)
eqn.Lasso.OLS <- paste("TARGET_WINS =", paste(coef.Lasso.OLS[1],paste(paste(signs.Lasso.OLS[2:length(signs.pca.step)],Betas.Lasso.OLS),collapse = " ")))

```

The following model is fit :

** `r eqn.Lasso.OLS`**

Like the Simple model, this is more or less interpretable with Batting 2B variable being insigniticant and sign of Fielding DP being counter intuitive. The R-Squared is about 33%. A good candidate model.

```{r}
# Using Variable into OLS regression

summary(Lasso.OLS.Fit)
par(mfrow = c(2,2))
plot(Lasso.OLS.Fit,which = 1:4)
# Lasso.Step.Fit <- stepAIC(Lasso.OLS.Fit, scope = list(lower = ~1, upper = formula(Lasso.OLS.Fit)),direction = 'both')
# summary(Lasso.Step.Fit)
# plot(Lasso.Step.Fit)
# par(mfrow = c(1,2))
# plot(y = lasso.resid, x = lasso.pred,ylim = c(-40,40))
# plot(Lasso.Step.Fit$fitted.values, Lasso.Step.Fit$residuals,ylim = c(-40,40),xlim = c(45,110),xlab = "Fitted.OLS", ylab = "residuals")



# cordata$Pitch_H_Outlier <- as.numeric(cordata$Pitch_H_Outlier)
# summary(lm(data = cordata ,  TARGET_WINS ~ flag + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_BATTING_H  + TEAM_BASERUN_CS ))
```


### 4.6  Partial Least Squares (PLS) Regression

Like the PCA, PLS regression was attempted to increase the predictive performance by sacrificing interpretability. 

```{r, fig.cap="PLS component selection"}
plsdf <- data.frame(x,y)
# plsdf <- plsdf %>% dplyr::filter(Pitch_H_Outlier == F)
colnames(plsdf)[20] <- "TARGET_WINS"

pls.Fit <- pls::plsr(TARGET_WINS ~ ., data  = plsdf[,which(colnames(plsdf) %in% c("BatSO_Filter","PitSO_Filter"))*-1], scale = T, validation = "CV")

library(pls)
validationplot(pls.Fit)
#summary(pls.Fit)
library(caret)
# plot(varImp(pls.Fit),top = 10)
# plot(pls::R2(pls.Fit))

pls.df <- data.frame(pls.Fit$scores[,1:4],plsdf$TARGET_WINS)
colnames(pls.df)[5] <- "TARGET_WINS"

# test to make sure if the PLS does the OLS on the component scores 

pls.lm.Fit <- lm(data = pls.df,formula = TARGET_WINS ~ ., )
formula_pls <- as.character(formula(pls.lm.Fit))[3]
predictors_pls <- unlist(strsplit(formula_pls, split = "+",fixed = T))
coef.pls <- round(coefficients(pls.lm.Fit),2)
signs.pls <- ifelse(sign(coef.pls)==1,"+", "-")
Betas.pls <- paste(abs(coef.pls[2:length(coef.pls)]),"*",predictors_pls)
eqn.pls <- paste("TARGET_WINS =", paste(coef.pls[1],paste(paste(signs.pls[2:length(signs.pls)],Betas.pls),collapse = " ")))
```
The following model is fit 

**`r eqn.pls`**

This regression has the highest R-Squared so far of 43%. It will be interesting to evaluate the models against all the data.

```{r}

summary(pls.lm.Fit)
anova(pls.lm.Fit)
par(mfrow = c(2,2))
plot(pls.lm.Fit, which = 1:4)

ImpVars <- function(loadingsmatrix,k) {
  purrr::map(.x = 1:k,.f = function(x) {names(loadingsmatrix[order(abs(loadingsmatrix[,x]),decreasing = T),x])})
}




```


### 4.7 Random forest

Random forest method was used for feature selection and the resultant features were used to fit a OLS regression. 

```{r,fig.cap = "Variable importance" }
set.seed(7)
# load the library
library(mlbench)
library(caret)
control <- rfeControl(functions = rfFuncs, method = "cv",number = 10)

results <- rfe(cordata[,2:20],cordata[,1],sizes = c(1:19),rfeControl = control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))


form <- paste("TARGET_WINS ~",paste(c( "TEAM_FIELDING_E", "TEAM_BATTING_H", "TEAM_BATTING_BB", "TEAM_PITCHING_SO"),collapse = "+"))
rf.Fit <- lm(data =cordata,formul = form)
formula_rf <- as.character(formula(rf.Fit))[3]
predictors_rf <- unlist(strsplit(formula_rf, split = "+",fixed = T))
coef.rf <- round(coefficients(rf.Fit),2)
signs.rf <- ifelse(sign(coef.rf)==1,"+", "-")
Betas.rf <- paste(abs(coef.rf[2:length(coef.rf)]),"*",predictors_rf)
eqn.rf <- paste("TARGET_WINS =", paste(coef.rf[1],paste(paste(signs.rf[2:length(signs.rf)],Betas.rf),collapse = " ")))
```

The following model was fit

**`r eqn.rf`**


```{r}
summary(rf.Fit)




```


# 5. Imputing and Data Prep for testing

From section 4 it is seen that the variables that have missing data were deemed important. Hence we'll impute data using K nearest neighbor technique , with k = 5.  The models fit in above section are evaluated against the full data and summarized in the section below.

```{r}
MoneyBallDataPrep <- function(Path) {
  # Library calls
  library(dplyr)
  library(magrittr)
  library(VIM)
  
  # read data in
  MB2 <- read.csv(file = Path , header = T)
  
  # Identity columns positions of those that are not required.
  NotRequired <- c("INDEX", "TEAM_BATTING_HBP")
  NotRequiredPos <- which(colnames(MB2) %in% NotRequired)
  
  # Impute Data

  MB2 <- kNN(data = MB2,variable = colnames(MB2)[NotRequiredPos*-1], k = 5)
  
  # Attrubute and indicator variables
  
  MB2 <- MB2  %>% 
    mutate(BatHR_Filter = as.logical(ifelse(TEAM_BATTING_HR <= 59, 1,0))) %>% 
    mutate(BatSO_Filter = as.logical(ifelse(TEAM_BATTING_SO <= 250, 1,0))) %>% 
    mutate(PitSO_Filter = as.logical(ifelse(TEAM_PITCHING_SO > 2000, 1, 0))) %>% 
    mutate(Pitch_H_Outlier = as.logical(ifelse(TEAM_PITCHING_H >= 1890, 1,0)))
  
  Pitch_H_outlierT <- MB2$INDEX[MB2$Pitch_H_Outlier]
  BatSO_Filter_T <- MB2$INDEX[MB2$BatSO_Filter]
  PitSOFilter_T <- MB2$INDEX[MB2$PitSO_Filter]
  
  MB2 <- MB2 %>% 
    mutate (AltSlope = as.logical(ifelse(INDEX %in% setdiff(setdiff(Pitch_H_outlierT,BatSO_Filter_T), PitSOFilter_T), 1,0)))
  
 # Get required columns
  reqcols <- colnames(MB2)[c(grep("_imp",colnames(MB2))*-1,NotRequiredPos*-1)]
  
 MB2 %>% dplyr::select(reqcols)
}

```


```{r}
TestData <- MoneyBallDataPrep(Path = 'MoneyBall.csv')
TestData$Pitch_H_Outlier <- as.numeric(TestData$Pitch_H_Outlier)
# Test of Simple model

simple.Fitted <- predict(simple.Fit, newdata = TestData)
MAE_Simple.Fit <- mean(abs(TestData$TARGET_WINS - simple.Fitted))
MSE_Simple.Fit <- mean((TestData$TARGET_WINS - simple.Fitted)^2)
AIC_Simple.Fit <- AIC(simple.Fit)
AdjRSquared_Simple.Fit <- summary(simple.Fit)$adj.r.squared

# Test of Simple.rev.Fit
simple.rev.fitted <- predict(simple.rev.Fit,newdata = TestData)
#plot(TestData$TARGET_WINS , simple.rev.fitted)
MAE_Simple.rev.Fit <- mean(abs(TestData$TARGET_WINS - simple.rev.fitted))
MSE_Simple.rev.Fit <- mean((TestData$TARGET_WINS - simple.rev.fitted)^2)
AIC_Simple.rev.Fit <- AIC(simple.rev.Fit)
AdjRSquared_Simple.rev.Fit <- summary(simple.rev.Fit)$adj.r.squared

# Test of Simple Stepwise fit

simple.step.fitted <- predict(object = Simple.step.Fit,newdata = TestData)
MAE_simple.step.Fit <- mean(abs(TestData$TARGET_WINS - simple.step.fitted))
MSE_simple.step.Fit <- mean((TestData$TARGET_WINS - simple.step.fitted)^2)
AIC_simple.step.Fit <- AIC(Simple.step.Fit)
AdjRSquared_Simple.Step <- summary(Simple.step.Fit)$adj.r.squared

# Test of PCR 
xtest <- TestData[,pcavars]
#xtest$Pitch_H_Outlier <- as.logical(xtest$Pitch_H_Outlier)
pca.fitted <- predict(pca.Fit,newdata = xtest,ncomp = 6)
MAE_pca.Fit <- mean(abs(TestData$TARGET_WINS - pca.fitted))
MSE_pca.Fit <- mean((TestData$TARGET_WINS - pca.fitted)^2)
AIC_pca.Fit <- AIC(pca.step.Fit) # using a non mvr class for AIC calculation 
AdjRSquared_pca.Fit <- summary(pca.step.Fit)$adj.r.squared

# Test of predictors from Lasso fit.

lasso.OLS.Fitted <- predict(Lasso.OLS.Fit,newdata = TestData)
MAE_lasso.OLS.Fit <- mean(abs(TestData$TARGET_WINS - lasso.OLS.Fitted))
MSE_lasso.OLS.Fit <- mean((TestData$TARGET_WINS - lasso.OLS.Fitted)^2)
AIC_Lasso.OLS.Fit <- AIC(Lasso.OLS.Fit)
AdjRSquared_Lasso.OLS.Fit <- summary(Lasso.OLS.Fit)$adj.r.squared

# Test of PLS model
TestData.rev <- TestData %>% 
  mutate(Pitch_H_Outlier = as.numeric(Pitch_H_Outlier)) %>% 
  mutate(BatHR_Filter = as.numeric(BatHR_Filter)) %>% 
  mutate(AltSlope = as.numeric(AltSlope)) 
pls.Fitted <- predict(pls.Fit, newdata = TestData.rev, ncomp = 4)
MAE_pls.Fit <- mean(abs(TestData.rev$TARGET_WINS - pls.Fitted))
MSE_pls.Fit <- mean((TestData.rev$TARGET_WINS - pls.Fitted)^2)
AIC_pls.Fit <- AIC(pls.lm.Fit)
AdjRSquared_pls.Fit <- summary(pls.lm.Fit)$adj.r.squared

# random forest predictors

rf.Fitted <- predict(rf.Fit, newdata = TestData)
MAE_rf.Fit <- mean(abs(TestData.rev$TARGET_WINS - rf.Fitted))
MSE_rf.Fit <- mean((TestData.rev$TARGET_WINS - rf.Fitted)^2)
AIC_rf.Fit <- AIC(rf.Fit)
AdjRSquared_AIC_rf.Fit <- summary(rf.Fit)$adj.r.squared

ModelComparison <- data.frame(Type = c("Simple","Simple revised", "Stepwise", "Prin.Comp.Regression","Lasso Predictors", "Partial Least Squares", "Random forest predictors" ),
                              Formula = c(eqn.simple,eqn.simple.rev,eqn.simple.step,eqn.pca.step,eqn.Lasso.OLS,eqn.pls,eqn.rf),
                              Adj.R.Squared = c(AdjRSquared_Simple.Fit,AdjRSquared_Simple.rev.Fit,AdjRSquared_Simple.Step,AdjRSquared_pca.Fit,AdjRSquared_Lasso.OLS.Fit,AdjRSquared_pls.Fit,AdjRSquared_AIC_rf.Fit),
                              AIC = c(AIC_Simple.Fit,AIC_Simple.rev.Fit,AIC_simple.step.Fit,AIC_pca.Fit,AIC_Lasso.OLS.Fit,AIC_pls.Fit,AIC_rf.Fit),
                              MAE = c(MAE_Simple.Fit,MAE_Simple.rev.Fit,MAE_simple.step.Fit,MAE_pca.Fit,MAE_lasso.OLS.Fit,MAE_pls.Fit,MAE_rf.Fit))
```


### 6. Model Selection

```{r}
pander::pandoc.table(ModelComparison)
```

From the table above, the Simple revised model is the best canidate from the perspective of MAE and Simplicity. While the PLS model have the highest R-Squared, the MAE and interpretability is not sufficient for selection. 

## 7. Prediction

```{r}
PredictionData <- read.csv(file = "MoneyBall_Test.csv")
Processed.PredictData <- MoneyBallDataPrep("MoneyBall_Test.csv")
P_TARGET_WINS <- round(predict(simple.rev.Fit,newdata = Processed.PredictData),0)
Processed.PredictData$P_TARGET_WINS <- P_TARGET_WINS
write.csv(file = "MoneyBall_Test_Seshadri.csv",Processed.PredictData)
#hist(P_TARGET_WINS)
```


# Appendix

# A.1 Data dictionary

```{r, echo=F}
dictionary <- readxl::read_xlsx('DataDictionary_Baseball.xlsx', sheet = 1)
knitr::kable(dictionary)
```

# A.2 Code

```{r, ref.label= knitr::all_labels(), echo=TRUE, eval=FALSE, tidy = T}

```
