---
title: "Project1"
author: "Sri Seshadri"
date: "9/30/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
library(magrittr)
library(dplyr)
```

## 1. Introduction



## 2. Exploratory Data Analysis (EDA)


### 2.1 About the data


```{r}
# Read the data in
MB <- read.csv('MoneyBall.csv')
cols <- colnames(MB)
library(mosaic)
# Compute overall statistic and show missing values
sanitycheck <- do.call(rbind,dfapply(MB,favstats, select = is.numeric))
rowname <- rownames(sanitycheck)
sanitycheck <- sanitycheck %>%  mutate(cv = sd/mean)
rownames(sanitycheck) <- rowname
knitr::kable(round(sanitycheck,2), caption = "Summary Stats and missing values")
```

### 2.2 Missing data

Where are they from? Systemic pattern?

```{r, fig.width=8}
# indicator variables for record missing
MB <- MB %>% 
        mutate(Missing_BatSO = as.logical(ifelse(is.na(TEAM_BATTING_SO),1,0))) %>%
        mutate(Missing_BasSB = as.logical(ifelse(is.na(TEAM_BASERUN_SB),1,0))) %>% 
        mutate(Missing_BasCS = as.logical(ifelse(is.na(TEAM_BASERUN_CS),1,0))) %>% 
        mutate(Missing_PitSO = as.logical(ifelse(is.na(TEAM_PITCHING_SO),1,0))) %>% 
        mutate(Missing_FieldDP = as.logical(ifelse(is.na(TEAM_FIELDING_DP),1,0)))  
MB$Missing <- as.logical(ifelse(rowSums(MB[,18:22]) > 0 ,1, 0))
library(ggplot2)
# p0 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = Missing_BatSO, fill = Missing_BatSO)) + geom_histogram(alpha = 0.5)

p1 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = Missing, fill = Missing)) + geom_histogram(alpha = 0.5) + theme_bw()

p2 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_SO, color = Missing, fill = Missing)) + geom_histogram(alpha = 0.5) + theme_bw()
gridExtra::grid.arrange(p1,p2,ncol = 2)

```

### 2.3 Relationships among variables


```{r}
cordata <- MB %>% dplyr::select(cols[c(-1,-11)]) 
cordata <- cordata[complete.cases(cordata),]
corrplot::corrplot(cor(cordata),tl.cex = 0.8)
```
#### 2.3.1 Exploring the relationships further

What are the linear relationships with varying slopes - parallel lines?

```{r}
ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_H, y = TEAM_BATTING_H, color = Missing)) + geom_point()
ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_H, fill = Missing, color = Missing)) + geom_histogram(alpha = 0.5) + facet_grid(Missing ~ .)

MB <- MB %>% 
  dplyr::mutate(Pitch_H_Outlier = as.logical(ifelse(TEAM_PITCHING_H >= 1890, 1,0))) 
MB_test <- MB %>% filter(Missing == F)
  ggplot(data = MB_test,mapping = aes(x = TEAM_PITCHING_H, color = Pitch_H_Outlier, fill = Pitch_H_Outlier)) + geom_histogram(alpha = 0.5) + theme_bw()
  
ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_H, y = TEAM_BATTING_H, color = Pitch_H_Outlier )) + geom_point() + theme_bw()

ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_SO, y = TEAM_BATTING_SO, color = Pitch_H_Outlier )) + geom_point() + theme_bw()

ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_BB, y = TEAM_BATTING_BB, color = Pitch_H_Outlier )) + geom_point() + theme_bw()

ggplot(data = MB_test, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = Pitch_H_Outlier )) + geom_point() + theme_bw()



```


#### 2.3.2  Indicator variables 

```{r}
MB <- MB  %>% 
    mutate(BatHR_Filter = as.logical(ifelse(TEAM_BATTING_HR <= 59, 1,0))) %>% 
    mutate(BatSO_Filter = as.logical(ifelse(TEAM_BATTING_SO <= 250, 1,0))) %>% 
    mutate(PitSO_Filter = as.logical(ifelse(TEAM_PITCHING_SO > 2000, 1, 0)))

I1 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_HR, color = BatHR_Filter, fill = BatHR_Filter)) + geom_histogram(alpha = 0.5) + theme_bw()
I2 <- ggplot(data = MB, mapping = aes(x = TEAM_BATTING_SO, color = BatSO_Filter, fill = BatSO_Filter)) + geom_histogram(alpha = 0.5) + theme_bw()
gridExtra::grid.arrange(I1,I2,ncol = 2, nrow = 2)
 #MB  %>%  dplyr::filter(Missing == T) %>% 
#ggplot(mapping = aes(x = TEAM_PITCHING_SO, fill = PitSO_Filter, color = PitSO_Filter )) + geom_histogram(alpha = 0.5) + theme_bw()
colnames(MB)[1] <- "INDEX"
Pitch_H_outlierT <- MB$INDEX[MB$Pitch_H_Outlier]
BatSO_Filter_T <- MB$INDEX[MB$BatSO_Filter]
PitSOFilter_T <- MB$INDEX[MB$PitSO_Filter]

MB <- MB %>% mutate (AltSlope = as.logical(ifelse(INDEX %in% setdiff(setdiff(Pitch_H_outlierT,BatSO_Filter_T), PitSOFilter_T), 1,0)))

MB$HRSO_Filter <- as.logical(ifelse(rowSums(MB[,c('BatHR_Filter','BatSO_Filter')]) > 0 ,1, 0))


I3 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = Pitch_H_Outlier )) + geom_point() + theme_bw()
I4 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color = AltSlope )) + geom_point() + theme_bw()
I5 <- ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = AltSlope)) + geom_point() + theme_bw()
I6 <- ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = BatSO_Filter)) + geom_point() + theme_bw()
I7 <- ggplot(data = MB, mapping = aes(y = TEAM_BATTING_SO, x = TEAM_PITCHING_SO, color = PitSO_Filter)) + geom_point() + theme_bw()
I8 <- ggplot(data = MB, mapping = aes(x = TEAM_PITCHING_HR, y = TEAM_BATTING_HR, color =  BatSO_Filter)) + geom_point() + theme_bw()
gridExtra::grid.arrange(I3,I4,I5,I6,I7,I8, ncol = 2, nrow = 3)
```

```{r}
reqcols <- colnames(MB)[c(-1,-11,-18:-23,-29)]
cordata <- MB %>% dplyr::select(reqcols) 
reqrows <- complete.cases(cordata)
cordata <- cordata[reqrows,]
# remove the lower end of the Batting SO and outliers in Pitching SO

cordata <- cordata %>% 
            dplyr::filter(BatSO_Filter == !T) %>% 
            dplyr::filter(PitSO_Filter == !T)
ggplot(data = cordata, mapping = aes(y = TARGET_WINS, x = TEAM_BATTING_HR, color = AltSlope)) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
ggplot(data = cordata, mapping = aes(y = TARGET_WINS, x = TEAM_BATTING_HR, color = as.factor(Pitch_H_Outlier))) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
ggplot(data = cordata, mapping = aes(x = TEAM_FIELDING_E, y = TARGET_WINS, color = as.factor(Pitch_H_Outlier))) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
ggplot(data = cordata, mapping = aes(x = TEAM_FIELDING_E, y = TARGET_WINS, color = AltSlope)) + geom_point() + theme_bw()+ geom_smooth(method = "lm")
```

## 3.0 Feature selections

What are the important features amongst complete cases?



Would a simple model work?

```{r}

simple.Pred <- colnames(cordata)[c(1:9,15:17)]
simple.Fit <- lm(formula = TARGET_WINS ~ . , data = cordata[,simple.Pred])
summary(simple.Fit)
car::vif(simple.Fit)
plot(simple.Fit)
plot(simple.Fit,which = 4)
# length(simple.Pred)

## Generating equation
coef.simple <- round(coefficients(simple.Fit),2)
signs.simple <- ifelse(sign(coef.simple)==1,"+", "-")
Betas.simple <- paste(abs(coef.simple[2:length(coef.simple)]),"*",simple.Pred)
eqn.simple <- paste("TARGET_WINS =", paste(coef.simple[1],paste(paste(signs.simple[2:length(signs.simple)],Betas.simple),collapse = " ")))


simple.rev.Pred <- simple.Pred[which(simple.Pred %in% c("TEAM_BASERUN_CS","Pitch_H_Outlier")) * -1]
simple.rev.Fit <- lm(formula = TARGET_WINS ~ . , data = cordata[,simple.rev.Pred])
summary(simple.rev.Fit)
car::vif(simple.rev.Fit)
plot(simple.rev.Fit)
plot(simple.rev.Fit,which = 4)

## Generating equation
coef.simple.rev <- round(coefficients(simple.rev.Fit),2)
signs.simple.rev <- ifelse(sign(coef.simple.rev)==1,"+", "-")
Betas.simple.rev <- paste(abs(coef.simple.rev[2:length(coef.simple.rev)]),"*",simple.rev.Pred[-1])
eqn.simple.rev <- paste("TARGET_WINS =", paste(coef.simple.rev[1],paste(paste(signs.simple.rev[2:length(signs.simple.rev)],Betas.simple.rev),collapse = " ")))

```
### 3.1 Automated Variable selection

```{r}
Simple.step.Fit <- MASS::stepAIC(simple.rev.Fit, scope = list(lower = ~1, upper = formula(simple.rev.Fit)),direction = "both",trace = F)
summary(Simple.step.Fit)
plot(Simple.step.Fit)

formula_simple.step <- as.character(formula(Simple.step.Fit)) [3]
Simple.step.pred <- unlist(strsplit(formula_simple.step, split = "+",fixed = T))
coef.simple.step <- round(coefficients(Simple.step.Fit),2)
signs.simple.step <- ifelse(sign(coef.simple.step)==1,"+", "-")
Betas.simple.step <- paste(abs(coef.simple.step[2:length(coef.simple.step)]),"*",Simple.step.pred)
eqn.simple.step <- paste("TARGET_WINS =", paste(coef.simple.step[1],paste(paste(signs.simple.step[2:length(signs.simple.step)],Betas.simple.step),collapse = " ")))
```


### 3.2 PCA

```{r}

# cleanup data for PCA

nearzeros <- caret::nearZeroVar(cordata,saveMetrics = T)
zerovars <- rownames(nearzeros)[nearzeros$zeroVar]
pcavars <- colnames(cordata)[c(1,which(colnames(cordata) %in% zerovars))*-1]
pcadata <- cordata[,pcavars]

pca <- princomp(pcadata,cor = T)

summary(pca)
# x <- model.matrix(TARGET_WINS ~., data = cordata)[,-1]
# colnames(x)[15:19] <- c("Pitch_H_Outlier","Bat_HR_Filter","BatSO_Filter", "PitSO_Filter","flag")
# pca <- princomp(x[,-17:-18],cor = T)
# summary(pca)
cumvariance <- cumsum(pca$sdev^2) / sum(pca$sdev^2)
plot(cumvariance, xlab = "Component Number", ylab = "Cumulative Variance", type = "l")
points(cumvariance)

pca.scores <- pca$scores[,1:6]
#df <- data.frame(pca.scores,MB[reqrows,colnames(MB)[c(2,24)]])
dfclust <- data.frame(pca.scores,cordata[,c('Pitch_H_Outlier','BatHR_Filter','AltSlope', 'TARGET_WINS')])
dfclust <- dfclust %>% dplyr::mutate(cluster = as.factor(Pitch_H_Outlier + BatHR_Filter/10))
#colnames(df)[7] <- c("TARGET_WINS")
clust1 <- ggplot(data = dfclust, mapping = aes(x = Comp.1, y = Comp.2, color = cluster )) + geom_point() + theme_bw()
clust2 <- ggplot(data = dfclust, mapping = aes(x = Comp.1, y = Comp.2, color = AltSlope)) + geom_point() + theme_bw()
library("RColorBrewer")
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
sc <- scale_colour_gradientn(colours = myPalette(100))
clust3 <- ggplot(data = dfclust, mapping = aes(x = Comp.1, y = Comp.2, color = TARGET_WINS)) + geom_point() + theme_bw() + scale_colour_gradient2() + sc
gridExtra::grid.arrange(clust1,clust2,clust3, ncol=2)

library(MASS)
lower.lm <- lm(data = dfclust[,c(1:6,10)], formula = TARGET_WINS ~ 1)
upper.lm <- lm(data = dfclust[,c(1:6,10)], formula = TARGET_WINS ~ .)
pca.step.Fit <- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "both", trace = F)
summary(pca.step.Fit)
anova(pca.step.Fit)
plot(pca.step.Fit)


pca.forward.Fit <- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "forward", trace = F)
summary(pca.forward.Fit)

plot(pca.forward.Fit)

pca.backward.Fit<- stepAIC(lower.lm, scope = list(upper = formula(upper.lm), lower = ~1), direction = "backward", trace = F)
summary(pca.backward.Fit)
plot(pca.backward.Fit)

# The below code just replicates the pca.step.Fit for easy prediction for testing. Its a change in class object.

pcadatarev <- data.frame(TARGET_WINS = cordata[,1],pcadata)
pca.Fit <- pls::pcr(data = pcadatarev, formula = TARGET_WINS ~ .,scale = T,ncomp = 6)

formula_pca <- as.character(formula(pca.step.Fit))[3]
predictors_pca <- unlist(strsplit(formula_pca, split = "+",fixed = T))
coef.pca.step <- round(coefficients(pca.step.Fit),2)
signs.pca.step <- ifelse(sign(coef.pca.step)==1,"+", "-")
Betas.pca.step <- paste(abs(coef.pca.step[2:length(coef.pca.step)]),"*",predictors_pca)
eqn.pca.step <- paste("TARGET_WINS =", paste(coef.pca.step[1],paste(paste(signs.pca.step[2:length(signs.pca.step)],Betas.pca.step),collapse = " ")))

```

### 3.2 Lasso

```{r}
library(glmnet)
x <- model.matrix(TARGET_WINS ~., data = cordata)[,-1]
colnames(x) <- gsub(pattern = "TRUE", "", colnames(x))
y <- cordata$TARGET_WINS
grid = 10^seq(10,-2,length = 100)
lasso.out <- cv.glmnet(x,y,alpha = 1)
plot(lasso.out)
lasso.Fit <- glmnet(x,y,alpha = 1, lambda = lasso.out$lambda.min)
#summary(lasso.Fit)
Candidate_Lasso <- names(coef(lasso.Fit)[order(abs(round(coef(lasso.Fit),3)),decreasing = T),])[2:10]
Coeff.Lasso <- coef(lasso.Fit)[order(abs(round(coef(lasso.Fit),3)),decreasing = T)][2:10]
Candidate_Lasso <- gsub(pattern = "TRUE",replacement = "",x = Candidate_Lasso)
lasso.formula <- paste("TARGET_WINS ~ ",paste(Candidate_Lasso, collapse = " + "))

ImpPredictors.Lasso <- data.frame(Predictors = Candidate_Lasso,Importance = abs(Coeff.Lasso))

library(forcats)
ggplot(data = ImpPredictors.Lasso,mapping = aes(y = Importance, x = fct_reorder(Predictors,Importance))) + geom_col(color = 'red', fill = 'red') + coord_flip() + theme_bw() + xlab("Predictors")
# Proper naming of variables


# R Squared
lasso.Fit$dev.ratio

lasso.pred <- predict(lasso.Fit,s = lasso.out$lambda.min,newx =x)
lasso.resid <- y - as.numeric(lasso.pred)
plot(y = lasso.resid, x = lasso.pred)
qqnorm(lasso.resid)
qqline(lasso.resid)

# Using Variable into OLS regression
Lasso.OLS.Fit <- lm(data = cordata[,c('TARGET_WINS',Candidate_Lasso)], formula = TARGET_WINS ~ .)
summary(Lasso.OLS.Fit)

Lasso.Step.Fit <- stepAIC(Lasso.OLS.Fit, scope = list(lower = ~1, upper = formula(Lasso.OLS.Fit)),direction = 'both')
summary(Lasso.Step.Fit)
plot(Lasso.Step.Fit)
par(mfrow = c(1,2))
plot(y = lasso.resid, x = lasso.pred,ylim = c(-40,40))
plot(Lasso.Step.Fit$fitted.values, Lasso.Step.Fit$residuals,ylim = c(-40,40),xlim = c(45,110),xlab = "Fitted.OLS", ylab = "residuals")

formula_Lasso.OLS <- as.character(formula(Lasso.OLS.Fit))[3]
predictors_Lasso.OLS <- unlist(strsplit(formula_Lasso.OLS, split = "+",fixed = T))
coef.Lasso.OLS <- round(coefficients(Lasso.OLS.Fit),2)
signs.Lasso.OLS <- ifelse(sign(coef.Lasso.OLS)==1,"+", "-")
Betas.Lasso.OLS <- paste(abs(coef.Lasso.OLS[2:length(coef.Lasso.OLS)]),"*",predictors_Lasso.OLS)
eqn.Lasso.OLS <- paste("TARGET_WINS =", paste(coef.Lasso.OLS[1],paste(paste(signs.Lasso.OLS[2:length(signs.pca.step)],Betas.Lasso.OLS),collapse = " ")))


# cordata$Pitch_H_Outlier <- as.numeric(cordata$Pitch_H_Outlier)
# summary(lm(data = cordata ,  TARGET_WINS ~ flag + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_BATTING_H  + TEAM_BASERUN_CS ))
```


### 3.3 Partial Least Squares (PLS) Regression

How to add indicator variables to the model?

```{r}
plsdf <- data.frame(x,y)
# plsdf <- plsdf %>% dplyr::filter(Pitch_H_Outlier == F)
colnames(plsdf)[20] <- "TARGET_WINS"

pls.Fit <- pls::plsr(TARGET_WINS ~ ., data  = plsdf[,which(colnames(plsdf) %in% c("BatSO_Filter","PitSO_Filter"))*-1], scale = T, validation = "CV")

library(pls)
validationplot(pls.Fit)
summary(pls.Fit)
library(caret)
plot(varImp(pls.Fit),top = 10)
plot(pls::R2(pls.Fit))

pls.df <- data.frame(pls.Fit$scores[,1:4],plsdf$TARGET_WINS)
colnames(pls.df)[5] <- "TARGET_WINS"

# test to make sure if the PLS does the OLS on the component scores 

pls.lm.Fit <- lm(data = pls.df,formula = TARGET_WINS ~ ., )

plot(pls.lm.Fit)
summary(pls.lm.Fit)
anova(pls.lm.Fit)

ImpVars <- function(loadingsmatrix,k) {
  purrr::map(.x = 1:k,.f = function(x) {names(loadingsmatrix[order(abs(loadingsmatrix[,x]),decreasing = T),x])})
}

formula_pls <- as.character(formula(pls.lm.Fit))[3]
predictors_pls <- unlist(strsplit(formula_pls, split = "+",fixed = T))
coef.pls <- round(coefficients(pls.lm.Fit),2)
signs.pls <- ifelse(sign(coef.pls)==1,"+", "-")
Betas.pls <- paste(abs(coef.pls[2:length(coef.pls)]),"*",predictors_pls)
eqn.pls <- paste("TARGET_WINS =", paste(coef.pls[1],paste(paste(signs.pls[2:length(signs.pls)],Betas.pls),collapse = " ")))


```


### 3.4 Random forest

```{r}
set.seed(7)
# load the library
library(mlbench)
library(caret)
control <- rfeControl(functions = rfFuncs, method = "cv",number = 10)

results <- rfe(cordata[,2:20],cordata[,1],sizes = c(1:19),rfeControl = control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))


form <- paste("TARGET_WINS ~",paste(c( "TEAM_FIELDING_E", "TEAM_BATTING_H", "TEAM_BATTING_BB", "TEAM_PITCHING_SO"),collapse = "+"))

rf.Fit <- lm(data =cordata,formul = form)
summary(rf.Fit)

```


### 4. Imputing and Data Prep for testing

```{r}
MoneyBallDataPrep <- function(Path) {
  # Library calls
  library(dplyr)
  library(magrittr)
  library(VIM)
  
  # read data in
  MB2 <- read.csv(file = Path , header = T)
  
  # Identity columns positions of those that are not required.
  NotRequired <- c("INDEX", "TEAM_BATTING_HBP")
  NotRequiredPos <- which(colnames(MB2) %in% NotRequired)
  
  # Impute Data

  MB2 <- kNN(data = MB2,variable = colnames(MB2)[NotRequiredPos*-1], k = 5)
  
  # Attrubute and indicator variables
  
  MB2 <- MB2  %>% 
    mutate(BatHR_Filter = as.logical(ifelse(TEAM_BATTING_HR <= 59, 1,0))) %>% 
    mutate(BatSO_Filter = as.logical(ifelse(TEAM_BATTING_SO <= 250, 1,0))) %>% 
    mutate(PitSO_Filter = as.logical(ifelse(TEAM_PITCHING_SO > 2000, 1, 0))) %>% 
    mutate(Pitch_H_Outlier = as.logical(ifelse(TEAM_PITCHING_H >= 1890, 1,0)))
  
  Pitch_H_outlierT <- MB2$INDEX[MB2$Pitch_H_Outlier]
  BatSO_Filter_T <- MB2$INDEX[MB2$BatSO_Filter]
  PitSOFilter_T <- MB2$INDEX[MB2$PitSO_Filter]
  
  MB2 <- MB2 %>% 
    mutate (AltSlope = as.logical(ifelse(INDEX %in% setdiff(setdiff(Pitch_H_outlierT,BatSO_Filter_T), PitSOFilter_T), 1,0)))
  
 # Get required columns
  reqcols <- colnames(MB2)[c(grep("_imp",colnames(MB2))*-1,NotRequiredPos*-1)]
  
 MB2 %>% dplyr::select(reqcols)
}

```


```{r}
TestData <- MoneyBallDataPrep(Path = 'MoneyBall.csv')
# Test of Simple.rev.Fit
simple.rev.fitted <- predict(simple.rev.Fit,newdata = TestData)
#plot(TestData$TARGET_WINS , simple.rev.fitted)
MAE_Simple.rev.Fit <- mean(abs(TestData$TARGET_WINS - simple.rev.fitted))
MSE_Simple.rev.Fit <- mean((TestData$TARGET_WINS - simple.rev.fitted)^2)
AIC_Simple.rev.Fit <- AIC(simple.rev.Fit)
AdjRSquared_Simple.rev.Fit <- summary(simple.rev.Fit)$adj.r.squared

# Test of Simple Stepwise fit

simple.step.fitted <- predict(object = Simple.step.Fit,newdata = TestData)
MAE_simple.step.Fit <- mean(abs(TestData$TARGET_WINS - simple.step.fitted))
MSE_simple.step.Fit <- mean((TestData$TARGET_WINS - simple.step.fitted)^2)
AIC_simple.step.Fit <- AIC(Simple.step.Fit)
AdjRSquared_Simple.Step <- summary(Simple.step.Fit)$adj.r.squared

# Test of PCR 
xtest <- TestData[,pcavars]
pca.fitted <- predict(pca.Fit,newdata = xtest,ncomp = 6)
MAE_pca.Fit <- mean(abs(TestData$TARGET_WINS - pca.fitted))
MSE_pca.Fit <- mean((TestData$TARGET_WINS - pca.fitted)^2)
AIC_pca.Fit <- AIC(pca.step.Fit) # using a non mvr class for AIC calculation 
AdjRSquared_pca.Fit <- summary(pca.step.Fit)$adj.r.squared

# Test of predictors from Lasso fit.

lasso.OLS.Fitted <- predict(Lasso.OLS.Fit,newdata = TestData)
MAE_lasso.OLS.Fit <- mean(abs(TestData$TARGET_WINS - lasso.OLS.Fitted))
MSE_lasso.OLS.Fit <- mean((TestData$TARGET_WINS - lasso.OLS.Fitted)^2)
AIC_Lasso.OLS.Fit <- AIC(Lasso.OLS.Fit)
AdjRSquared_Lasso.OLS.Fit <- summary(Lasso.OLS.Fit)$adj.r.squared

# Test of PLS model
TestData.rev <- TestData %>% 
  mutate(Pitch_H_Outlier = as.numeric(Pitch_H_Outlier)) %>% 
  mutate(BatHR_Filter = as.numeric(BatHR_Filter)) %>% 
  mutate(AltSlope = as.numeric(AltSlope)) 
pls.Fitted <- predict(pls.Fit, newdata = TestData.rev, ncomp = 4)
MAE_pls.Fit <- mean(abs(TestData.rev$TARGET_WINS - pls.Fitted))
MSE_pls.Fit <- mean((TestData.rev$TARGET_WINS - pls.Fitted)^2)
AIC_pls.Fit <- AIC(pls.lm.Fit)
AdjRSquared_pls.Fit <- summary(pls.lm.Fit)$adj.r.squared

# random forest predictors

rf.Fitted <- predict(rf.Fit, newdata = TestData)
MAE_rf.Fit <- mean(abs(TestData.rev$TARGET_WINS - rf.Fitted))
MSE_rf.Fit <- mean((TestData.rev$TARGET_WINS - rf.Fitted)^2)
AIC_rf.Fit <- AIC(rf.Fit)
AdjRSquared_AIC_rf.Fit <- summary(rf.Fit)$adj.r.squared

ModelComparison <- data.frame(Type = c("Simple", "Stepwise", "Prin.Comp.Regression","Lasso Predictors", "Partial Least Squares", "Random forest predictors" ),
                              Formula = c(as.character(formula(simple.rev.Fit)),as.character(formula(Simple.step.Fit)),as.character(formula(pca.step.Fit)),as.character(formula(Lasso.OLS.Fit)),as.character(formula(pls.lm.Fit)), as.character(formula(rf.Fit))),
                              Adj.R.Squared = c(AdjRSquared_Simple.rev.Fit,AdjRSquared_Simple.Step,AdjRSquared_pca.Fit,AdjRSquared_Lasso.OLS.Fit,AdjRSquared_pls.Fit,AdjRSquared_AIC_rf.Fit),
                              AIC = c(AIC_Simple.rev.Fit,AIC_simple.step.Fit,AdjRSquared_pca.Fit,AdjRSquared_Lasso.OLS.Fit,AdjRSquared_pls.Fit,AdjRSquared_AIC_rf.Fit),
                              MAE = c(MAE_Simple.rev.Fit,MAE_simple.step.Fit,MAE_pca.Fit,MAE_lasso.OLS.Fit,MAE_pls.Fit,MAE_rf.Fit))
```


### 5. Testing models

```{r}
pander::pandoc.table(ModelComparison)
```


