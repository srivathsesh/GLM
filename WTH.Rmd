---
title: "Wine sales prediction"
author: "Sri Seshadri"
date: "11/11/2017"
output: 
    pdf_document:
      toc : true
      toc_depth: 4
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F)
library(magrittr)
source('getequation.R')
```


\pagebreak

# 1. Introduction

A large wine manufacturer is studying data collected on 12,000 commercially available wines, with a goal to predict the number of cases ordered based upon the characteristics. The manufacturer intends to adjust the winde offerings based on the findings. The data collected is related to chemical properties of wine with response variable being the number of sample cases sold to distribution companies. the data also includes features like review stars provided by the tasters and label appeal. 

# 2. Exploratory Data analysis

The below table shows the summary statistics of the data. It is seen that there are missing data. "Stars" has the most missing values. The missing values mostly correspond to NO (0) cases sold. Which is most likely due to lack of opportunity to sample because of none sold.

```{r}
winedata <- read.csv(file = 'wine.csv')
library(mosaic)
# Compute overall statistic and show missing values
sanitycheck <- do.call(rbind,dfapply(winedata,favstats, select = is.numeric))
rowname <- rownames(sanitycheck)

rownames(sanitycheck) <- rowname
knitr::kable(round(sanitycheck,2), caption = "Summary Stats and missing values")
Missing.Stars <- as.factor(ifelse(is.na(winedata$STARS),1,0))
Missing.Alch <- as.factor(ifelse(is.na(winedata$Alcohol),1,0))
Missing.Sulph <- as.factor(ifelse(is.na(winedata$Sulphates),1,0))
Missing.pH <- as.factor(ifelse(is.na(winedata$Sulphates),1,0))
Missing.TSD <- as.factor(ifelse(is.na(winedata$TotalSulfurDioxide),1,0))
Missing.FSD <- as.factor(ifelse(is.na(winedata$FreeSulfurDioxide),1,0))
Missing.Chl <- as.factor(ifelse(is.na(winedata$Chlorides),1,0))
Missing.ResSug <- as.factor(ifelse(is.na(winedata$ResidualSugar),1,0))

winedataeda <- cbind(winedata, Missing.ResSug,Missing.Chl,Missing.FSD,Missing.TSD,Missing.pH,Missing.Sulph,Missing.Alch,Missing.Stars)

# Write to file for examination

# write.csv(file = 'winedatamod.csv', winedata)

```


```{r, fig.cap="Missing STARS values' association with Number of cases sold", fig.height=4}
library(ggplot2)
p <- ggplot(data = winedataeda) 
p + geom_bar(mapping = aes(x = TARGET, col = Missing.Stars, fill = Missing.Stars)) + theme_classic()

#winedata %>% group_by(TARGET) %>% summarise_at("STARS", function(x) sum(is.na(x))/length(x)*100)
```

Figure 2 shows the historgram of features in the data. The chemical properties of wines appear to share an identical distribution with peaks closer to zero. This mau be likely to some standardization done to the day. 
The variable "TARGET" looks to be poisson or negative biniomially distributed with inflation at 0. 


```{r,fig.cap="Histograms of features"}
windedataMelt <- reshape2::melt(winedata[,-1])
ggplot(data = windedataMelt,mapping = aes(x = value)) + geom_histogram() + facet_wrap(~variable,scales = "free_x") + theme_bw()
```

\pagebreak

# 3. Feature selection

In this section we'll attempt to select important features that explain the target variable. We'll explore the correlations that might exist in the data. There is positive correlations between TARGET and STARS and LabelAppeal. 

```{r, fig.cap="Correlation plot"}
wine.complete <- winedata[complete.cases(winedata),]
cordata <- cor(wine.complete)
corrplot::corrplot(cordata, tl.cex = 0.7)

winedata <- winedata %>% mutate(TARGET0 = as.factor(ifelse(TARGET > 0 ,0, 1)))
```

It'll be useful to identify what contributes to the zero inflation in the TARGET variable. For which let's create an indicator variable "TARGET0" to equal 1 when TARGET is 0 and 0 otherwise. 

## 3.1 Decision Trees

### 3.1.1 Predictors for zero cases sold (No sale)

Figure 4 shows the decision tree for TARGET0. Where 1 is no sale (cases sold = 0) and 0 is sale (cases sold > 0 ). Figure 5 shows the variable importance plot after a random forest bootstrap. While LabelAppeal did not contribute to node purity (Decrease in Gini index), it did affect accuracy on out of bag (OOB) samples. 

```{r, fig.cap= "Decision tree - Sale (0) or no Sale (1)"}
predictors <- colnames(winedata)[c(-1,-2,-17)]
response <- c("TARGET","TARGET0")
tr <- rpart::rpart(formula = TARGET0 ~ ., data = winedata[,c(predictors, response[2])], method = "class")
library(partykit)
plot(partykit::as.party(tr), gp = gpar(fontsize = 8 ))
```

```{r, fig.cap= "Variable Importance - Randomforest"}
randtr <- randomForest::randomForest(formula = TARGET0 ~ ., data = winedata[complete.cases(winedata),c(predictors, response[2])], importance = T)
randomForest::varImpPlot(randtr,main = 'Variable Importance plot - Random forest')
```


### 3.1.2 Predictors for cases sold; when successfuly sold (cases > 0)

Figure 6 shows the decision tree of cases sold when they are greater than 0. It can be seen that the LabelAppeal and STARS are the top hitters. Figure 7 shows the variable importance plot when a random forest method is employed, where a random set of predictors are chosen at each iteration to fit a decision tree. Variables whose exclusion contributes to higher Mean Squared Error (MSE) is deemed important. LabelApeal, STARS and Alcohol are top 3 variables that are important.

```{r, fig.cap="Simple tree for TARGET"}
windedata.filtered <- winedata %>% filter(TARGET0 == 0)
tr2 <- rpart::rpart(formula = TARGET ~ ., data = windedata.filtered[,c(predictors, response[1])])
plot(as.party(tr2),gp = gpar(fontsize = 8 ))
```


```{r, fig.cap="Variable importance plot - TARGET" }
randtr2 <- randomForest::randomForest(formula = TARGET ~ ., data = windedata.filtered[complete.cases(windedata.filtered),c(predictors, response[1])], importance = T)
randomForest::varImpPlot(randtr2, main = "Variable importance plot from random forest - TARGET")
```

\pagebreak

# 4. Training and validation samples

The data is split into training and validation sample for evaluating models for final selection. The 75% of the data is sampled as training sample and the rest is set aside as validation set. Figure 8 shows that the TARGET variable is identically distributed for both training and validation samples.

```{r, fig.cap="TARGET distribution for training and validation samples"}
set.seed(10)
train <- winedata %>%  dplyr::sample_frac(0.75)
test <- winedata[!winedata$INDEX %in% train$INDEX,]
par(mfrow = c(1,2))
hist(train$TARGET, main = "TARGET from training sample", xlab = "TARGET", col = "red")
hist(test$TARGET, main = "TARGET from test sample", xlab = "TARGET", col = "blue")


```

## 4.1. The problem.

The  TARGET variable from both training and validation (test) sample are associated with missing values of key predictors like STARS. GLM models in R ignores missing records for model fitting. In which case the zero inflation in the histograms in Figure 8 may not be representative in the training sample. Figure 9 illustrates the problem.

```{r, fig.cap="Histograms of TARGET when missing values of STARS is ignored"}
issue.train <- train %>% filter(!is.na(STARS)) 
issue.test <- test %>% filter(!is.na(STARS))
par(mfrow = c(1,2))
hist(issue.train$TARGET, col = "red", main = "Histogram of TARGET - Training", xlab = "TARGET")
hist(issue.test$TARGET, col = "blue", main = "Histogram of TARGET - Test", xlab = "TARGET")
```

## 4.2 Problem Aleviation

From figure 1 it is inferred that most of the missing STARS are attributed to TARGET of 0. This implies that since no cases were distributed, it is likely that the wines did not have the opportunity to be tasted for review stars. Therefore we will impute zeros to STARS when cases distributed (TARGET) is zero.

```{r}
train <- train %>% mutate(STARS = ifelse(is.na(STARS) & TARGET == 0,0,STARS))
test <- train %>% mutate(STARS = ifelse(is.na(STARS) & TARGET == 0,0,STARS))
```


# 5. Modeling

The following modeling approaches will be tried for predicting the number of cases sold.

* Poisson regression
* Negative binomial regression
* Zero infalted Poisson regression
* Zero inflated Negative binomial regression
* OLS regression

## 5.1 Poisson regression.

As seen in table 1, the mean and standard deviation of TARGET are not too far apart. Poisson model is a good candidate for regression modeling.  

### 5.1.1 Simple model with Label appeal as the predictor.

While we know that the TARGET variable is zero inflated and there are atleast more than one important predictor from the section above, we'll attempt to build the model ground up with LabelAppeal as the single predictor. 

```{r}
poisson.mdl1 <- glm(TARGET ~ LabelAppeal,family = poisson(link = 'log'), data = train)
summary(poisson.mdl1)
lmtest::lrtest(poisson.mdl1)
glance.poisson.mdl1 <- broom::glance(poisson.mdl1)
glance.poisson.mdl1$pseudoR.Sq <- (poisson.mdl1$null.deviance - poisson.mdl1$deviance)/poisson.mdl1$null.deviance
poisson.mdl1.predict <- predict(poisson.mdl1,newdata = subset(test,!is.na(test$LabelAppeal)), type = 'response')
poisson.mdl1.coefs <- broom::tidy(poisson.mdl1)
glance.poisson.mdl1$MAE <- mean(abs(test$TARGET[!is.na(test$TARGET)] - round(poisson.mdl1.predict,0)))
knitr::kable(glance.poisson.mdl1,digits = 2,caption = "Simple Poisson model statistics")


# poisson.mdl2 <- glm(TARGET ~ LabelAppeal + STARS + Alcohol,family = poisson(link = 'log'), data = train)
# summary(poisson.mdl2)
# 
# poisson.full <- glm(TARGET ~ .,family = poisson(link = 'log'), data = train[,c(-1,-17)] )
# summary(poisson.full)
# 1 -pchisq(deviance(poisson.mdl2)-deviance(poisson.full),11)

```

#### 5.1.1.1 Model interpretation

It can be seen from the Likelihood ratio test ab0ve, that the slope of LabelAppeal is NOT zero and an unit increase in label appeal increases by `r round(exp(poisson.mdl1.coefs[2,2]),0)`. However the model is not an adequate fit with residual deviance over degrees of freedom being much higher than 1 (in comparison to a saturated model). This is also reflected in the Pseudo R Squared value. 

The model does not predict 0s. As can be see from figure 9. 

```{r, fig.cap= "Simple poisson regression diagnostics",fig.height=6}
layout <- matrix(c(1,2,3,4,5,5), ncol = 2, byrow = T)
layout(layout)
plot(poisson.mdl1)

plot(poisson.mdl1$data$TARGET , round(poisson.mdl1$fitted.values,0), xlab = "TARGET", ylab = "Rounded fitted")
```

### 5.1.2 Poisson model with Label appeal & Stars as predictors

Now, we'll use Label appeal and stars as predictors and compare the model with the simple model in the above section.

```{r}

poisson.mdl2 <- glm(TARGET ~ LabelAppeal + STARS, family = poisson(link = "log"), data = train[!is.na(train$LabelAppeal),])
summary(poisson.mdl2)
lmtest::lrtest(poisson.mdl2)


glance.poisson.mdl2 <- broom::glance(poisson.mdl2)
glance.poisson.mdl2$pseudoR.Sq <- (poisson.mdl2$null.deviance - poisson.mdl2$deviance)/poisson.mdl2$null.deviance
poisson.mdl2.predict <- predict(poisson.mdl2,newdata = subset(test,!(is.na(test$LabelAppeal)|is.na(test$STARS))), type = 'response')
poisson.mdl2.coefs <- broom::tidy(poisson.mdl2)
glance.poisson.mdl2$MAE <- mean(abs(test[complete.cases(test[,c(14,16)]),2] - round(poisson.mdl2.predict,0)))
knitr::kable(glance.poisson.mdl2,digits = 2,caption = "Poisson model statistics with STARS & LabelAppeal as predictors")
```

```{r, fig.cap="Model diagnostics TARGET ~ LabelAppeal + STARS"}
layout(layout)
plot(poisson.mdl2)
plot(train[complete.cases(train[,c(14,16)]),2] , round(poisson.mdl2$fitted.values,0), xlab = "TARGET", ylab = "Rounded fitted")
```

#### 5.1.2.1 Model interpretation.

It is seen that the regression model is statistically significant from the Likelihood Ratio Test (LRT) and the regression coefficents are also significant. The MAE is 1.5 which is a marginally higher than the simple model with LabelAppeal as predictor. However the ratio of residual deviance to its degrees of freedom is closer to 1, which can be interpreted as poor model fit to the data. This model also does not fit the zero counts. 

### 5.1.3 Dropping one predcitor from model.

It'll be interesting to see if there is value in dropping one of the regressors from the model. Below, LRT is performed by dropping one regressor at a time from the model.

```{r}
drop1(poisson.mdl2, test = "Chisq")
```

From LRT, both Label Appeal and STARS appear significant. Dropping Label Appeal (kepping STARS as the only predictor) provides a marginal increase in model fit compared to dropping STARS( keeping Label Appeal as the only predictor). This interpretation is onsistent with the decision tree interpretation in the section 3.1.2. 

**The LRT test for model comparison may be biased, since the models may have different samples sizes due to missing data. However, LRT can be used as a guidance for model selection. It is examined in the below section just to be sure**


#### 5.1.3.1  Two models - A. STARS as predictor B. LabelAppeal and Interpretation.

Verifying the results in the above section to make sure the drop1 test is providing us the expected results. It can be seen that when the data is kept contant between the models, the results are as expected. The result for model B is different from that of in section 5.1.1, this is because the data used to fit the model is different. Based on AIC and MAE comparison Label Appeal seems to be a better predcitor so far, when using a Poisson model.

```{r}
poisson.mdl3 <- glm(TARGET ~ STARS,family = poisson(link = 'log'), data = train)
summary(poisson.mdl3)
lmtest::lrtest(poisson.mdl3)
glance.poisson.mdl3 <- broom::glance(poisson.mdl3)
glance.poisson.mdl3$pseudoR.Sq <- (poisson.mdl3$null.deviance - poisson.mdl3$deviance)/poisson.mdl3$null.deviance
poisson.mdl3.predict <- predict(poisson.mdl3,newdata = subset(test,!is.na(test$STARS)), type = 'response')
poisson.mdl3.coefs <- broom::tidy(poisson.mdl3)
glance.poisson.mdl3$MAE <- mean(abs(abs(test[complete.cases(test[,16]),2] - round(poisson.mdl3.predict,0))))
knitr::kable(glance.poisson.mdl3,digits = 2,caption = "Simple Poisson model statistics with STARTS as regressor")
```

```{r, fig.cap="Model diagnostics TARGET ~ STARS"}
layout(layout)
plot(poisson.mdl3)
plot(train[complete.cases(train[,16]),2] , round(poisson.mdl3$fitted.values,0), xlab = "TARGET", ylab = "Rounded fitted")
```

```{r}
poisson.mdl4 <- glm(TARGET ~ LabelAppeal, data = train[complete.cases(train[,c(14,16)]),], family = poisson(link = "log"))
summary(poisson.mdl4)

lmtest::lrtest(poisson.mdl4)
glance.poisson.mdl4 <- broom::glance(poisson.mdl4)
glance.poisson.mdl4$pseudoR.Sq <- (poisson.mdl4$null.deviance - poisson.mdl4$deviance)/poisson.mdl4$null.deviance
poisson.mdl4.predict <- predict(poisson.mdl4,newdata = subset(test,!is.na(test$STARS)), type = 'response')
poisson.mdl4.coefs <- broom::tidy(poisson.mdl4)
glance.poisson.mdl4$MAE <- mean(abs(abs(test[complete.cases(test[,16]),2] - round(poisson.mdl4.predict,0))))
knitr::kable(glance.poisson.mdl4,digits = 2,caption = "Simple Poisson model statistics with LabelAppeal as regressor; non missing cases for STARS and LabelAppeal")
```

```{r, fig.cap="Model diagnostics TARGET ~ LABEL APPEAL non missing data for STARS and LabelAppeal"}
layout(layout)
plot(poisson.mdl4)
plot(train[complete.cases(train[,16]),2] , round(poisson.mdl4$fitted.values,0), xlab = "TARGET", ylab = "Rounded fitted")
```

### 5.1.4 Poisson regression with LabelAppeal, STARS and Alcohol as predictors

In this section Alcohol is added as a preditor to the model in 5.1.2. Then LRT is performed by dropping one variable from the predictor. Though Alcohol is statistically significant, it does not add significant predictive power to the model. There is no significant reduction in MAE. Also we see that there is pattern in residuals based on the levels of the TARGET variable. It is to be noted that the predicted values are in log scale.

```{r}
poisson.mdl5 <- glm(TARGET ~ LabelAppeal + STARS + Alcohol, data = train[complete.cases(train[,c(13,14,16)]),], family = poisson(link = "log"))
summary(poisson.mdl5)

lmtest::lrtest(poisson.mdl5)
glance.poisson.mdl5 <- broom::glance(poisson.mdl5)
glance.poisson.mdl5$pseudoR.Sq <- (poisson.mdl5$null.deviance - poisson.mdl5$deviance)/poisson.mdl5$null.deviance
poisson.mdl5.predict <- predict(poisson.mdl5,newdata = test[complete.cases(test[,c(13,14,16)]),], type = 'response')
poisson.mdl5.coefs <- broom::tidy(poisson.mdl5)
glance.poisson.mdl5$MAE <- mean(abs(abs(test[complete.cases(test[,c(13,14,16)]),2] - round(poisson.mdl5.predict,0))))
knitr::kable(glance.poisson.mdl5,digits = 2,caption = "Poisson model statistics with LabelAppeal, STARS, ALCOHOL as regressors")
```


```{r, fig.cap="Model diagnostics TARGET ~ LABEL APPEAL + STARS + Alcohol"}
layout(layout)
plot(poisson.mdl5, col =train[complete.cases(train[,c(13,14,16)]),2] )
plot(train[complete.cases(train[,c(13,14,16)]),2] , round(poisson.mdl5$fitted.values,0), xlab = "TARGET", ylab = "Rounded fitted")
```
#### 5.1.4.1 Model interpretation

```{r}
drop1(poisson.mdl5, test = "Chisq")


```

From LRT by droping one predictor at every iteration, it is seen that the exclusion of Alcohol does not affect the fit considerably. Hence it is best to drop Alcohol from the model.

### 5.1.5 Addition of AcidIndex as predictor

One of the other predictors that was found key in section 3.1.2 is AcidIndex. It is seen below that though Acidindex's coefficient is statsitically significant, it does not improve prediction. Alcohol's coefficient is marginally significant.  

```{r}
poisson.mdl6 <- glm(TARGET ~ LabelAppeal + STARS + Alcohol + AcidIndex, data = train[complete.cases(train[,c(13,14,15,16)]),], family = poisson(link = "log"))
summary(poisson.mdl6)

lmtest::lrtest(poisson.mdl6)
glance.poisson.mdl6 <- broom::glance(poisson.mdl6)
glance.poisson.mdl6$pseudoR.Sq <- (poisson.mdl6$null.deviance - poisson.mdl6$deviance)/poisson.mdl6$null.deviance
poisson.mdl6.predict <- predict(poisson.mdl6,newdata = test[complete.cases(test[,c(13,14,15,16)]),], type = 'response')
poisson.mdl6.coefs <- broom::tidy(poisson.mdl6)
glance.poisson.mdl6$MAE <- mean(abs(abs(test[complete.cases(test[,c(13,14,15,16)]),2] - round(poisson.mdl6.predict,0))))
knitr::kable(glance.poisson.mdl6,digits = 2,caption = "Poisson model statistics with LabelAppeal, STARS, ALCOHOL, AcidIndex as regressors")
```

```{r}
drop1(poisson.mdl6, test = "Chisq")
```

drop1 test shows that the exclusion of Alcohol provides the best AIC. In the next section we will fit a model with LabelAppeal, AcidIndex and STARS as predictors.

#### 5.1.5.1 Removal of Alcohol and inclusion of AcidIndex

The model with LabelAppeal, STARS and AcidIndex is fit below and it is seen that all the predictor variables' coefficients are statistically significant. However the MAE is not improved. LRT when one predictor is left out of the model at each iteration shows that inclusion of AcidIndex does NOT impact the AIC considerably.

```{r}
poisson.mdl7 <- glm(TARGET ~ LabelAppeal + STARS + AcidIndex, data = train[complete.cases(train[,c(14,15,16)]),], family = poisson(link = "log"))
summary(poisson.mdl7)
lmtest::lrtest(poisson.mdl7)
glance.poisson.mdl7 <- broom::glance(poisson.mdl7)
glance.poisson.mdl7$pseudoR.Sq <- (poisson.mdl7$null.deviance - poisson.mdl7$deviance)/poisson.mdl7$null.deviance
poisson.mdl7.predict <- predict(poisson.mdl7,newdata = test[complete.cases(test[,c(14,15,16)]),], type = 'response')
poisson.mdl7.coefs <- broom::tidy(poisson.mdl7)
glance.poisson.mdl7$MAE <- mean(abs(abs(test[complete.cases(test[,c(14,15,16)]),2] - round(poisson.mdl7.predict,0))))
knitr::kable(glance.poisson.mdl7,digits = 2,caption = "Poisson model statistics with LabelAppeal, STARS, ALCOHOL, AcidIndex as regressors")

```

#### 5.1.5.1 Model interpretation 

Droping one predictor and performing the LRT, it can be seen below that removing AcidIndex does NOT move the needle on AIC. 
```{r}
drop1(poisson.mdl7, test = "Chisq")
```

### 5.1.6 Poisson regression summary


```{r}
poisson.models <- list(poisson.mdl1, poisson.mdl2, poisson.mdl3, poisson.mdl4,poisson.mdl5,poisson.mdl6,poisson.mdl7)
poisson.equations <- sapply(poisson.models,function(x) getequation("TARGET",x, rounding = 3))
poisson.summary <- cbind(model = poisson.equations,rbind(glance.poisson.mdl1,glance.poisson.mdl2,glance.poisson.mdl3,glance.poisson.mdl4,glance.poisson.mdl5,glance.poisson.mdl6,glance.poisson.mdl7))
pander::pandoc.table(poisson.summary,digits = 2,caption = "Poisson fit summary",style = "grid")
```

The poisson regression fits is summarized in the table above. The models 

          `r getequation("TARGET",poisson.mdl2,rounding =3 )`
          `r getequation("TARGET",poisson.mdl3,rounding =3 )`
          
seems to be a reasonable model without running the risk of overfitting. While the model that includes AcidIndex to the above model seems marginally better, but requires another predictor for relatively similar MAE. However none of the models predict "no sales" well. Zero inflation poisson model would be useful and is explored in the next section.


## 5.2 Zero-Inflation Poisson regression (ZIP) model

The challenge we faced in the above section is to predict the zeros cases sold. In this section the predictors that were identified for zero cases sold in section 3.1.1 (figure 5) will be used as predictors to predict no sale and predictors that were identified in section 3.1.2 (figure 7) would be used to predict cases sold when there was a sale.

### 5.2.1 Zero Inflated Poisson (ZIP) model.

Figure 5 shows STARS has the maximum affect on accuracy of predicting a sale (TARGET > 0) or no sale (TARGET = 0). STARS is used as predictor for predicting zero sale and for a start,STARS and LabelAppeal would be used for predicting TARGET > 0.  

```{r, fig.cap="ZIP - rounded residuals vs TARGET"}
poisson.zif.mdl1 <- pscl::zeroinfl(TARGET ~ LabelAppeal+ STARS|STARS , data = train )
summary(poisson.zif.mdl1)
par(mfrow = c(1,2))
plot(poisson.zif.mdl1$model$TARGET,poisson.zif.mdl1$model$TARGET - round(poisson.zif.mdl1$fitted.values,0), xlab = "TARGET", ylab = "Residual (rounded)", main = "Residual vs TARGET")
abline(0,0,lty = 2,col = "red")
hist(poisson.zif.mdl1$model$TARGET - round(poisson.zif.mdl1$fitted.values,0), xlab = 'residuals',main = "histogram of residuals- ZIP",col = "black")

```

### 5.2.2 ZIP model's surprising result

The logistic component of the ZIP model shows that STARS is not a statistically significant contributor. However randomforest approach tells a different story. Also figure 14 shows that the residuals of the model increases as the TARGET value increases. In the next section we will attempt to verify this by modeling using the Logistic Hurdle model.


### 5.2.3 Logistic Hurdle model to verify ZIP

The same input to the model as provided in the ZIP model is supplied to the logistic regression. STAR is used as a predictor to predict when NO cases were sold. It is seen that STARS are good predictor of no sale. And most of the imputed data for stars was associated with TARGET = 0. The imputation has no effect on STARS being a good predictor. Here missing data itself is a signal more so than a noise.

Figure 15 shows the ROC curve of of logictic regression of both the training (black) and test data (red). The lines are on top of each other and both have the same AUC of 95%. 

LabelAppeal and STARS are used for predicting the counts in poisson model.


```{r,fig.cap = "ROC curve for No sale prediction"}
tst <- glm(TARGET0 ~ STARS,family = binomial(link = "logit"), data = train,subset = !is.na(train$STARS) )
summary(tst)
drop1(tst,test = "Chisq")
rocCurve.train.logis <- pROC::roc(response = train[!is.na(train$STARS),"TARGET0"], predictor = tst$fitted.values)
rocCurve.test.logis <- pROC::roc(response = test[!is.na(train$STARS),"TARGET0"],predictor = predict(tst,newdata = test[!is.na(test$STARS),],type = "response"))

plot(rocCurve.train.logis, asp = NA, legacy.axes = T)
plot(rocCurve.test.logis, legacy.axes = T, asp = NA, add = T, col = "red")
legend(0.2,0.6,paste("AUC = ",round(pROC::auc(rocCurve.train.logis),2)))

```



```{r, fig.cap="Verify ZIP with Logistic Hurdle model"}

poisson.hurdle.mdl1 <- glm(TARGET ~ LabelAppeal + STARS, family = poisson(link = "log"), data = train[ train$TARGET0 == 0 & !is.na(train$STARS),])
summary(poisson.hurdle.mdl1)
drop1(poisson.hurdle.mdl1, test = "Chisq")
probsale <- predict(tst,newdata = train[ train$TARGET0 < 1 & !is.na(train$STARS),],type = "response")
predictedcount <- (1-probsale)*poisson.hurdle.mdl1$fitted.values
# plot(tst$fitted.values)
# plot(poisson.mdl2$fitted.values)
# plot(poisson.zif.mdl1$residuals)
#plot(train[train$TARGET0 < 1 & !is.na(train$STARS),2],train[ train$TARGET0 < 1 & !is.na(train$STARS),2]-round(predictedcount,0))

predictedCount2 <- (1 -predict(tst,newdata = train,type = "response") ) * predict(poisson.hurdle.mdl1, newdata = train, type = "response")
par(mfrow = c(1,2))
plot(train$TARGET,train$TARGET - round(predictedCount2,0), xlab = "TARGET", ylab = "Residuals (rounded)")
hist(train$TARGET - round(predictedCount2,0), xlab = "Residuals (rounded)",col = "black")

test.PredictedCount <- (1 -predict(tst,newdata = test,type = "response") ) * predict(poisson.hurdle.mdl1, newdata = test, type = "response")
MAE.zif.mdl1 <- mean(abs(test$TARGET - round(test.PredictedCount,0)),na.rm = T)
glance.poisson.zif.mdl1 <- data.frame(LogLik = logLik(poisson.zif.mdl1),AIC = AIC(poisson.zif.mdl1), BIC = BIC(poisson.zif.mdl1), MAE = MAE.zif.mdl1)
knitr::kable(glance.poisson.zif.mdl1, caption = "TARGET ~ LabelAppeal+ STARS|STARS")
```

It is seen that Figure 16 is identical to figure 14. ZIP model is verified. However I find that the ZIP model's logistic component not very intuitive to interpret. I am hoping that the professor Mickelson would be able to throw more light on this through email response of my question to him on 11/19/2017. 


#### 5.2.3.1 Model interpretation

From the drop1 test for logistic regression above, we see that dropping STARS would not increase AIC significantly. This warrants a fit with just LabelAppeal as predictor. Also it is seen that the residuals are skewed right, as the TARGET increases. There is bias in the model. 

The model makes physical sense, one would expect the cases shipped to be low if the rating stars are low. The log odds of cases shipped < 0 increases with increase in STARS. The theory also holds for the poisson component of the regression model.


### 5.2.4 ZIP with LabelAppeal for count prediction and STARS for No sale prediction

In this section, dropping of STARS in the poisson model in the above section is explored.

```{r,fig.cap='ZIP TARGET ~ LabelAppeal|STARS'}
poisson.zif.mdl2 <- pscl::zeroinfl(TARGET ~ LabelAppeal|STARS , data = train )
summary(poisson.zif.mdl2)
par(mfrow = c(1,2))
plot(poisson.zif.mdl2$model$TARGET,poisson.zif.mdl2$model$TARGET - round(poisson.zif.mdl2$fitted.values,0), xlab = "TARGET", ylab = "Residual (rounded)", main = "Residual vs TARGET")
abline(0,0,lty = 2,col = "red")
hist(poisson.zif.mdl2$model$TARGET - round(poisson.zif.mdl2$fitted.values,0), xlab = 'residuals',main = "histogram of residuals- ZIP",col = "black")

MAE.zif.mdl2 <- mean(abs(test$TARGET - predict(poisson.zif.mdl2,newdata = test,type = "response")),na.rm = T)

glance.poisson.zif.mdl2 <- data.frame(LogLik = logLik(poisson.zif.mdl2),AIC = AIC(poisson.zif.mdl2), BIC = BIC(poisson.zif.mdl2), MAE = MAE.zif.mdl2)
knitr::kable(glance.poisson.zif.mdl2, caption = "TARGET ~ LabelAppeal|STARS", digits = 2)
```

### 5.2.4.1 Model interpretation

The Figure 17 shows that the residuals are left tailed. The model does not predict zeros well. For goodness of fit assessment, a model is fit for samples where counts of sale is greater than 0. The residula deviance over degrees of freedom is less than 1 indicating a good fit.

```{r}
poisson.hurdle.mdl2 <- glm(TARGET ~ LabelAppeal, family = poisson(link = "log"), data = train[ train$TARGET0 == 0 & !is.na(train$STARS),])
summary(poisson.hurdle.mdl2)
```
### 5.2.5 Zero Inflation Poisson Regression summary

Below is the summary for ZIP models. While both models are comparable, and since both needs LabelAppeal and STARS, one could choose the first model with a marginal better performance in MAE.

```{r}
zip.models <- list(poisson.zif.mdl1,poisson.zif.mdl2)
zip.equations <- sapply(zip.models,function(x) getequation("TARGET",x,rounding = 2))
zip.summary <- cbind(model = zip.equations,rbind(glance.poisson.zif.mdl1,glance.poisson.zif.mdl2))
pander::pandoc.table(zip.summary,style = "grid",digits = 2,caption = "ZIP fit summary - Note model equation before | is for count and after | is prob of No sale")
```

\pagebreak

## 5.3 Negative Binomial models.

In this section Negative binomial model fits are explored.


### 5.3.1 Negative binomial fit with LabelAppeal and STARS as predictors.

Below is the results for the negative binomial fit, the results are identical to that of poisson models with a slight increase in standard errors. 

```{r, fig.cap="Negative binomial model",fig.height=4}
nb.mdl1 <- MASS::glm.nb(formula = TARGET ~ STARS + LabelAppeal, data = train)
summary(nb.mdl1)
par(mfrow= c(2,2))
plot(nb.mdl1, col = train$TARGET0)
drop1(nb.mdl1, test = "Chisq")
nb.predict.mdl1 <- predict(nb.mdl1,newdata = test,type = "response")
MAE.nb.mdl1 <- mean(abs(test$TARGET - nb.predict.mdl1),na.rm = T)
glance.nb.mdl1 <- broom::glance(nb.mdl1)
glance.nb.mdl1$MAE <- MAE.nb.mdl1

knitr::kable(glance.nb.mdl1, caption = "negative binomial model",digits = 2)
```

#### 5.3.1.1 Model interpretation

The model is identical to poisson model with the same predictors. Label Appeal has a marginal effect compared to STARS.

## 5.4. Zero inflated Negative binomial model

In this section a Zero inflated negative binomial model is fit. 

```{r,fig.cap="Zero inflated negative binomial model",fig.height=4}
zinb.mdl1 <- pscl::zeroinfl(TARGET ~ LabelAppeal+ STARS|STARS , data = train, dist = "negbin",EM = T)
summary(zinb.mdl1)
par(mfrow = c(1,2))
plot(zinb.mdl1$model$TARGET,round(zinb.mdl1$model$TARGET-zinb.mdl1$fitted.values,0), xlab = "TARGET", ylab = "Rounded residuals")
hist(round(zinb.mdl1$model$TARGET-zinb.mdl1$fitted.values,0),xlab = "Rounded residuals", main = "Histogram of residuals (rounded)")
```



```{r}
predict.zinb.mdl1 <- predict(zinb.mdl1,newdata = test, type = "response")
MAE.zinb.mdl1 <- mean(abs(test$TARGET - predict.zinb.mdl1),na.rm = T)
zinb.equation <- getequation("TARGET",zinb.mdl1)
glance.zinb.mdl1 <- data.frame(LogLik = logLik(zinb.mdl1),AIC = AIC(zinb.mdl1), BIC = BIC(zinb.mdl1), MAE = MAE.zinb.mdl1)
zinb.summary <- cbind(model = zinb.equation,glance.zinb.mdl1)
```


#### 5.4.1 Zero inflated negative binomial summary and interpretation

The model is identical to ZIP. The interpretation doesn't change from the ZIP model. The summary is shown in the table below. 

```{r}

pander::pandoc.table(zinb.summary, caption = "Zero inflated negative binomial model", digits = 2, style = "grid")
```

## 5.5 OLS regression

Lastly, an OLS regression is fit to the data to see how it performs.

```{r,fig.cap="OLS regression diagostics"}
lm.mdl1 <- lm(TARGET ~ STARS + LabelAppeal, data = train)
summary(lm.mdl1)
drop1(lm.mdl1, test = "F")
par(mfrow = c(2,2))
plot(lm.mdl1)
predict.lm.mdl1 <- predict(lm.mdl1, newdata = test)
MAE.lm.mdl1 <- mean(abs(predict.lm.mdl1 - test$TARGET), na.rm = T)
glance.lm.mdl1 <- broom::glance(lm.mdl1)
lm.equation <- getequation("TARGET",lm.mdl1)
glance.lm.mdl1$MAE <- MAE.lm.mdl1
summary.lm.mdl1 <- cbind(model = lm.equation, glance.lm.mdl1)
```

#### 5.5.1 model interpretation and summary

The model surprisingly beats expectation. The coefficients make logical sense and can definitely be a candidate model. The summary table is shown below. The MAE is comprarble to ZIP model above. There are patterns in the residuals (Figure 20), making it not structly meeting OLS assumptions.

```{r}
pander::pandoc.table(summary.lm.mdl1[,c("model","adj.r.squared","AIC","BIC","MAE")], caption = "OLS regression", style = "grid")
```

# 6. Model selection

Comparing the model summaries in Section 5, the Zero inflated binomial model:

*`r as.character(zip.summary$model[1])`*

has the minimum MAE. While STARS alone may have been sufficient as predictors as in the model:

*`r as.character(zip.summary$model[2])`*

given that STARS could be missing in new data sets, Label Appeal may be a handy predictor. It is surprising that Alcohol did not come out as an important predictor. It was expected that too high or too high proof (measure of alcohol by volume) may not be a popular choice for customers.


# 7. Model deployment

Along with this report an addendum file is provided that contains the R code for model deployment.

# 8. Conclusion

More than any chemical properties Label appeal and review stars are dominant factors in wine sales. Care in label appeal is as much required as in production of wine. It is recommended tha the wine manufacturer market wine with a catchy label to receive as many review stars as possible.   


